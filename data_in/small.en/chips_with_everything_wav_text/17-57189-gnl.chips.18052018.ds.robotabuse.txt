 The Guardian We're getting more and more used to talking to robots. I'm Siri, your virtual assistant. Personal assistants like the one on your smartphone or in the home like Amazon Echo are growing in popularity. And kids love them. A friend of mine has two young sons who regularly call for their Echo to play their favourite music or tell them a joke. And every now and then, she worries about the possible consequences of her son's learning to bark orders at something designed to sound like a human. Especially when, although you can usually change the settings, most of these personal assistants default to a female voice. In last week's episode, we mentioned in our Tech Fact of the Week that Google has introduced a new feature to the Google Assistant, designed to encourage kids to be more polite in their interactions with their friendly home robot, by rewarding them for saying please by saying things like, Thanks for asking so nicely. And it turns out, they're not the only ones who think kids might need a little encouragement to stop them from abusing machines, verbally or even physically. Children show abusive behaviour like blocking its path or dragging or even kicking the robot for fun. But do we need to be worried about the way that we learn to interact with robots? Is there any risk that it'll impact how we interact with other humans? And does that change when companies like Google are also trying to make their artificial intelligences sound as deceptively human as possible? The agent is not a human being, it does not have the capacities, the beliefs, the deep knowledge, all of that of a human being. And so by speaking with a human voice, it's actually in some sense a deception. And it's misleading people into what it is they might be dealing with. I'm Jordan Erika Webber, and this is Chips with Everything. Children push boundaries. They're natural experimenters. If something moves or makes sounds, they want to see how far it will go, how loud it can get and, inevitably, what it'll take to break it. Now, scientists from the Navalabs in Seoul, South Korea have taken that concept and designed a robot that capitalises on the natural curiosity of children to teach them to treat it with care. It looks like a giant tortoise, and its name is Shelly. My producer Danielle Stevens spoke to one of the minds behind Shelly, Hyun-Zin Gu. A Shelly is a robot interacting with children while restraining children's robot abusing behaviours. Shelly has a tortoise-like friendly appearance and touch-based simple and versatile interface, which encourages children to interact with the robot spontaneously in an environment such as kindergarten. Why did you and your team decide to build Shelly? We designed Shelly mainly for two reasons. Firstly, we wanted to make a social robot capable of one-to-many interaction with children because most current social robots are only capable of one-to-one interaction. Also, it is observed that service robots are abused by people, especially by children. This behaviour hampers the robot's mission and frustrates its social acceptance. Children show abusive behaviour on the around, like blocking its path or dragging or even kicking the robot for fun. Thus, our robot Shelly is designed to interact with children while achieving educational purposes in terms of refraining robot abusing. Is this not just a case of children being children? With all toys, they have this destructive path. Or do you think there's something more psychological about why children abuse robots? There are some research going on about why children abuse robots. In that research, they mainly said the first reason is curiosity. Tell me a little bit about the technology behind Shelly. How did you create it? We carefully designed Shelly's design concept. For example, the previous research has shown that escaping abused situations by the verbal warning is not effective because it turns out it only encourages children's curiosity more. Therefore, the robot should use another mechanism that not only gives negative feedback to children's abusive behaviours, but also maintain children's interest to the robot. The turtle's light shape was the perfect choice to implement these functions. Its friendly appearance and slow behaviour are perfect for interaction targeting children, and also its retracting motion can be implemented into restraining children's abusive behaviours by giving negative feedback. So what is it being used for now? Is it in schools? Is it just in your lab and children are visiting you? How is Shelly being used? Actually it's one of its legs broken after the final field test, so it's on repairing. So after the repairing, it's gonna be displayed at the Science Museum in Korea. Oh wow, okay. How many children have met Shelly? The Shelly was made during the neighbour lab's internship, and the Shelly met children in one session of monthly events held by neighbour labs. So at this day, employees can bring their children to the headquarters, and their children can meet Shelly and play with Shelly. So on average, about 100 children came for the event every month. So what age group are you testing Shelly on, or was Shelly tested on? Actually any children can play with Shelly, however, for their research on how Shelly restrained the children's abusive behaviour, Shelly was tested with children in age 6 to 13. Okay, and what was the general reaction to Shelly? So most children are very curious about how Shelly reacts to their actions. So as Shelly hides its heads and limbs inside, some children like to hit Shelly. But at the same time, if Shelly hides, it suspends all its interactions. So some children feel unhappy about those abusing actions, because they wanted to interact with Shelly. So in the end, children generally tried not to abuse Shelly, and even more, they actively suspended or discouraged other children's abusing behaviours. So why do children need to learn not to abuse robots? Because there's no doubt that many social robots will appear to our society soon, right? So people need to know, they need to treat well robots in their lives. This is kind of too futuristic perspective, but we need to treat everything in a good manner, not only for robots, but also for people around us interacting with us. So Shelly was created to prepare kids for a future in which they will regularly interact with social robots. But it's not just humans who are adapting to this possible future. The AIs we interact with now are, like the Google Assistant, being designed to sound more and more like human beings, because, the tech giants say, that's what users prefer. After the break, we'll talk to a professor of spoken language processing, who perceives this looming future of robots that sound just like us, and doesn't like what he sees. There's a shallowness to their knowledge and to their abilities, and that potentially creates a serious problem for users who might make totally the wrong assumptions about what they're dealing with. We'll be right back. If it didn't kill people, it is a fantastic mineral. It is a fantastic additive to any construction project that you're undertaking, except that it does kill people. Asbestos, the wonder material that has infiltrated buildings everywhere in Britain and beyond, protecting infrastructure against fire and making it stronger. It was the perfect solution for a post-war country, until it wasn't. Even though the health risks of asbestos exposure are now well established, countries continue to build with this material. But why? The mantra from the asbestos corporations were it saves more people than it kills. And so then it's worth the sacrifice, worth the risk. To find out more, head over to theguardian.com forward slash podcasts, or search Science Weekly on your podcast app. Welcome back to Chips with Everything. I'm Jordan Erica Webber. Before the break, we spoke to Hyunjin Gu about Shelley, the robotic tortoise that she and her team designed as an aid to teach children not to abuse their robot companions, now or in the future. So we've heard about attempts to teach humans how to interact physically with robots, but I wanted to know more about how we talk to them. So I'm Professor Roger Moore. I'm a professor of spoken language processing here at the University of Sheffield, and I'm part of the speech and hearing research group that we have here, which is one of the large groups in the UK that works on what we call speech technology. And these days I'm very concerned with how we interact with machines using voice and how that is integrated with other ways that we might interact with machines. And particularly with robots is an aspect of what I'm looking at just at the moment. So have you come across research about how humans speak to robots or to artificial intelligence? Well there's a huge amount of work, and it's growing substantially because, as you're probably aware, the underlying technology for speech-based systems has just improved dramatically in the last few years. So it's not so long ago that if you spoke to a speech recognizer, then it would probably get quite a large part of what you've said wrong. But that has all changed. And so that means that there's a whole new world opening up now where we can envisage speech-based interaction with so-called intelligent artefacts. So given that we can almost have conversations with robots now, what interests does that raise for academics? I mean I've just said how there have been dramatic improvements in the underlying technologies. So there are three main technologies involved, automatic speech recognition. Then the opposite to that is speech synthesis technology, so that's taking some representation that's inside the machine and turning it back into speech, so allowing the machine to talk back. And then the third sort of core technology is what we call the underlying dialogue manager. So all three of those technologies have improved dramatically, but there are still some serious questions about each of them. In the past, the way that machines spoke was rather simplified, the quality was not great, but you're probably aware that even in recent times the voices have become very natural, beginning to sound almost like a natural human being. But there are still a lot of research questions there about, well, yes, it can say a few things, but should it be more expressive? Should it have emotion? It's about giving the output voice of various accents or styles or personalities. All of that is still being looked at. But there's a whole other set of questions about the total system. You talked about a conversational interaction. Well, do we want to have conversational interactions with artifacts which are basically not another human being, but which are purporting to be? Will users know what to do in that situation? We know how to talk to people, but do we know how to talk to a machine? Because these systems are not people. There's a shallowness to their knowledge and to their abilities. And that creates, it potentially creates a serious problem for users who might make totally the wrong assumptions about what they're dealing with. Yeah. Is there any research that suggests that kids who grow up interacting with AIs in this way might then end up interacting with other humans in that way? Not that I'm aware of, but I think it's all too early. These things have only appeared in the last couple of years. And I have to say, the technology is running ahead of some rather important ethical questions here. One of my bugbears is the fact that the voices that are used in these agents are all human-like. And my view is that is totally inappropriate. The agent is not a human being. It does not have the capacities, the beliefs, the deep knowledge, all of that of a human being. And so by speaking with a human voice, it's actually, in some sense, a deception. This is a very unpopular view, but it would be easily possible to alter the voice of one of these artificial agents so that it sounds robotic. Why do you think it is so popular to have these robots sound like humans? Talking to the sort of developers and the companies are putting these things out, I get the message back. This is what the users want. I don't know if you saw the recent coverage of Google's new duplex. Hello, how's it going out here? Hi, I'm calling to book a women's haircut for our client. I'm looking for something on May 3rd. Give me one second. How did you feel about that? I mean, it's an incredibly impressive piece of interaction, although they've only published a couple of examples. So I don't know whether those are the ones that worked. But I'm not entirely surprised because machine learning, deep learning, artificial neural nets have sort of revolutionized these spectacular technical developments in our field over the last few years. And so there was a sort of inevitability about using those techniques in interactive dialogue and integrating the kinds of ums and ahs and pauses, all the terrible things that I'm doing there. They're not bad speech. They are there because they're informative. For example, they let the listener know that your brain is having a planning problem and you haven't quite figured out what the rest of the sentence is yet. And listeners understand that. It's all very fluid. So it was impressive that it could do that. But of course, they immediately jumped into this area of ethical concern and lots of people, including myself, flagged up, hold on, you know, you're deceiving these people you're calling. I mean, it's bad enough at the moment we get, you know, unwanted calls at home. Usually you can detect very quickly that they're automated. And if you don't want it, you can put your phone down on it. But if you're faced with something which appears to be human or purporting to be human, do we want to be putting people in that position? I think not. Do you think there's any risk that very young children, if they're speaking to robots that do sound very human-like, they might come into situations where they fail to distinguish between a robot and a human? I think that's probably very unlikely because they will test the limits of the technology and they will soon discover that it is not actually a human being. I've certainly seen reports where kids have been interviewed and research we did ourselves here with a particular robot, which is childlike and it has an expressive face. And when we asked the kids, you know, are you aware this is a robot, they were incredibly dismissive of course. Mr Moore had a lot to say about why we should be careful not to create machines that sound enough like humans for us to use them to deceive each other. So I was curious what he thinks the future of human-robot interaction will look like. Will we adapt to a world in which we regularly interact with machines? Or will we change them to suit us? I think the answer is both of those. So people adapt, you know, you don't have to ask a person to adapt, people do adapt. So whatever technology is put in front of them, if they can find a value, find a use for it, they will do so and they will do whatever is needed to make it work. You know, spoken language interaction is also very deep-seated and we know when we're talking to people. It's only in this grey area where we're deceiving people that we get into problems. And then users won't know what to do. If they don't know that they're talking to a human or talking to a machine, then they will have difficulties. So it's not only the deception, but they may soon find that the system can't satisfy them in ways that they were expecting it could. So do you think then that there's really no risk that the way we interact with robots is going to affect how we interact with other humans, unless we have this situation where we have people making robots sound so much like humans that it gets confusing? I think it's, oh yeah, I agree. I think it's very, very unlikely. Spoken language is not just a bunch of commands that we've learned, you know, that we use when we're interacting with people. It's a beautifully orchestrated and integrated interaction. There's a lot more to interaction between people than just the voice. In keeping with the theme of this week's episode, our interesting tech fact of the week is all about artificial intelligence, and it has a celebrity twist. YouTube Red, the paid-for ad-free version of YouTube, is going to release a documentary series about AI, produced by Susan Downey and narrated by her husband, Tony Stark himself, Robert Downey Jr. I'd like to thank Hyunjin Gu and Professor Roger Moore for joining us this week. You can find a link to a video of Shelly, the robotic tortoise, in action in this week's episode description on the Guardian website. And remember, if you have any fun tech facts, questions or feedback on the show, and if you have any ideas for cool digital stories that we should cover in future episodes, email us at chibspodcastattheguardian.com. I'm Jordan Erica Webber. Thanks for listening.