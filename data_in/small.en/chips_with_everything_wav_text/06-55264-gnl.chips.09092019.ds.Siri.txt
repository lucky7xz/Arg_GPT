 The Guardian. Siri, are you always listening? I only listen when you're talking to me. At the end of July, The Guardian's UK tech editor and friend of this show, Alex Hern, broke a story that ruffled Apple's feathers. Human beings in offices around the world are listening to some Siri recordings without the explicit knowledge of those Siri users to try and improve the voice assistant. Those recordings often involve deeply personal information, including confidential medical details, ongoing drug deals, and people having sex. A whistleblower, who worked as a contractor hired to listen to various Siri recordings, contacted Alex to air their uneasiness at certain aspects of their job. The big part was simply that their job existed. Quite aside from all of the information they heard and all of the later stuff we would find out about accidental activations and the problems with how Siri is set up in practice, their main problem was just that they had a job that involved listening to things that people said to or around their iPhones, their Apple watches, their iPads, their HomePods and their Apple TVs, and they felt that that wasn't adequately disclosed. When the story was published, Apple told Alex that the data they collect from these recordings is used to help Siri and Dictation understand you better and recognise what you say. Everyone inside this industry seems to have assumed that saying, we will use your recordings to improve the product is inherently a disclosure of the fact that human beings will listen to the recordings. Outside of those offices, no one else thought that, and most people were pretty shocked to find that this microphone that they carry around in their pocket all the time is sending, as Apple says, less than 1% of activations to a human who's listening to it. A month later, Apple relented and apologised for not being clearer about their methods. As we've seen numerous times with other tech companies recently, Apple reviewed their policy and made some changes. One change in particular included ending the contracts of those hired to do this listening, which they call grading, in the first place. So yeah, the whistleblower who told me about this originally has lost their job, as have the other five whistleblowers that I've spoken to since. But does that mean the problem's fixed? I'm Jordan Erica Webber, and this week I look at the scandal that rocked the voice assistant world, and ask whether or not we can trust that voice assistants aren't eavesdropping on our most private moments. This is Chips with Everything. And for that, you need the audio file of that recording, but you also need the location that it's sent from, because you can't tell if Siri's given a good answer if you don't know where they are. You check the transcript, you check the reply, you say, yes, this is good or no, this is bad, and you move on. That's not an egregious privacy violation, although still you need consent and disclosure for it. And the problem comes when frequently you have accidental activations. Again, perhaps you want someone checking whether or not a particular activation was accidental or not. So this is whether or not you intended to say, hey, Siri, and then offer a question. Nothing triggered in the room here, but we may have just triggered it at home, sorry. It may be deliberate, you may be asking, or it may be accidental. The iPhone may have overheard something which it thinks is the wake word. Or if you have something like an Apple Watch, there are multiple ways that you can turn on Siri without saying the wake word. You can lift it up and speak into it, or you can press and hold the side button. All of these accidental activations are the source of the really egregious stuff. That seems to have been where we got these recordings of a drug deal in progress, where we got these recordings of a doctor talking to their patient, and where we got apparently frequent, according to the grader I spoke to, recordings of couples having sex. That also isn't helped by the fact that another thing that can trigger an accidental activation, according to this grader, is the sound of a zipper. I'm nervous to take my phone in my bedroom, everyone. I've started taking my watch off. Listening into the kinds of things people say to their phones must have been a very strange experience for the people who were listening in, like your whistleblower. Strange but also uncomfortable in more and less ways. Since the story was published, I've spoken to far more graders about this. One of the more upsetting stories I heard was someone who had what seemed to be someone describing sexual abuse of a child into Siri. The way the technology is built, there's no way that Apple can trace it back to the original user, but also it's not actionable in and of itself. It's speech. It's disturbing and upsetting, but it's not clear what one would do with this information. The graders said they reported it to Apple and were told that there's nothing that Apple could do. So when all this stuff came out, what was Apple's response? So for about a week, they dismissed the story. They didn't respond to it publicly except to confirm that grading occurred. They, as I understand it, privately were dismissing it to other reporters. They were telling people this isn't anything, you know, this is disclosed, this is standard practice. So after about a week, it became clear that that was an untenable response. The number of people who were simply stunned by the existence of the Siri grading program, let alone the effective privacy violations caused by experimental activations meant that Apple realized it had screwed up. Presumably its customers weren't particularly happy when they found out that this was happening. Yeah, I mean, the big thing that I think helped the story was a slightly uncouth bit of journalism from The Sun, which having read our story on the Friday, tracked down someone who works in one of Amazon's call centers and basically wrote exactly the same story about Alexa overhearing sex stuff, which they put on their front page. And in that way that sometimes there is a nice effect of having both tabloid and broadsheet media while we did the sort of high minded privacy violations and the philosophical problems with humans doing things that people might assume AI did. The Sun did, war, pervy tech companies are listening to you get your rocks off, which was a much simpler way of cutting perhaps to the same core. And kind of that one, two punch, I think really helped the story go quite wide. There was also there were a few moderately high profile in their own world interventions. John Gruber, a very influential Apple blogger who writes the blog Daring Fireball, didn't cover the story for the first three or four weeks. But then after a while wrote a very long piece going, I dismissed the story when it came out. But actually, the more I think about it, the more violating this is, this is bad, gross, and shouldn't be done by any company. And as a fan of Apple and someone who respects their privacy stance, it's particularly bad from Apple. This is a company that advertises in Las Vegas during a consumer electronics show, what happens on your iPhone stays on your iPhone, emphasizing that whereas Google sends loads of information back to the servers and Amazon sends loads of information back to the servers, the way the iPhone is built, it really does all stay on your iPhone, unless you're speaking to Siri, in which case what happens on your iPhone is sent to servers and call centers in Cork and Barcelona and elsewhere. And then humans listen to it. And you don't get told about that. Now when you spoke to Apple for the story, they said, quote, Siri responses are analyzed in secure facilities and all reviewers are under the obligation to adhere to Apple's strict confidentiality requirements. But your whistleblower doesn't seem to think that their confidentiality requirements are particularly strict. Am I right? So their problem was that not so much the confidentiality requirements weren't strict, but that there was no real vetting, that all it took to get the job was apply so that you can speak and write English and be a vaguely respectable person, and they felt that as someone who was handling deeply personal information, there should be perhaps a little bit more vetting, perhaps a criminal records check, perhaps, you know, a background check, perhaps anything beyond just, can you do the work for near minimum wage, which is the only real barrier that the contractors who were employed to do this seem to put up. There wasn't a criminal background check. Not as far as the grader was aware. I should say Apple is not the only company having issues with intrusive voice assistance. The new Google Home Mini has a massive problem and it isn't even out yet. It was released to reporters for free last week to test it out. One journalist discovered his device recorded everything in earshot and uploaded it. You thought you were home alone, right? Well, be careful what you are saying to Alexa. Amazon employees are reportedly eavesdropping on your conversation in order to help Alexa get better at her job. Joining us now is Robo. Let's do it in the timeline. So we've got initially back in April, Bloomberg broke the news that Amazon did this, that Amazon had a big old call center full of people listening to Alexa recordings. In July, Google was the next domino to fall in quite a dramatic fashion. A Belgian TV station not only reported on the existence of Google Home's call centers doing the same thing, but also reported that it had received a leak of a thousand audio files. So this was not only a big scoop, but also a massive data protection breach, which Google promptly shut down its European operation for this entirely while investigating how it happened. Google did publish a blog post which defended its practice of letting human employees listen to audio recordings of conversations between users and its Google Assistant software. In the post, they said, quote, We apply a wide range of safeguards to protect user privacy throughout the entire review process. Language experts only review around 0.2% of all audio snippets. Audio snippets are not associated with user accounts as part of the review process, and reviewers are directed not to transcribe background conversations or other noises and only to transcribe snippets that are directed to Google. They also highlighted that there is an option to turn off storing audio data to your Google account completely, but acknowledged they would be looking at ways to further clarify how data is used to improve speech technology. Then we had Apple, which we broke. Then after that, we had Microsoft, which is in many ways more egregious. Not only was the company's Cortana voice assistant having a human-led grading program, but also Skype, its voice chat service, has humans listening in if you turn on the translation feature. Not that many people use the translation feature, sure, but it's one step further again to be listening in to private phone calls between two people, which is what some Microsoft transcribers were doing. Then finally, Facebook with its messenger translated voice chat feature fell prey to the same thing. Some of these companies have promised changes, some of them have enacted changes. So Amazon has already turned on a switch that you can now turn off to disable this. Microsoft has promised it'll do something, but so far all it's done has changed its privacy policy. And after the break, we look at what Apple says it's done to fix the problem of its eavesdropping products, and why this fix might just be a plaster on a surgical wound. Microsoft guides them generally, which is building the quote unquote best products, pushing the technology forward, creating things that their competitors can't. That's always going to guide their action. And the problem is, it's hard to view that in a way that guards people's privacy. And it's very easy to view that in a way that pushes the boundaries of what you can exploit, what you can take, and what you can survey. We'll be right back. Welcome back to Tips With Everything. I'm Jordan Erica Webber. This week I'm chatting to the Guardian's UK tech editor Alex Hearn about a story he broke about intrusive voice assistants. He was informed by an anonymous contractor who had been hired to provide quality control, or grading, for Apple's Siri voice assistant. This often meant listening to private, in some cases very private, interactions. A month after the story broke, Apple apologized. They don't do this all that often, and it really is the first full-throated apology we've seen in a good five or six years from Apple. They accepted that they had simply done wrong, and that this program should not have been launched without the disclosure and without the options that had happened. Let's go through the announcement that Apple made then. So they committed to three changes. Number one, it will no longer keep audio recordings of Siri users by default, though it will retain automatically generated transcripts of the requests. Can you explain what that means in layman's terms, and is it a good thing for Apple users? Yes. So what it means is that there will no longer be a vast database of every Siri recording that has been made and kept by Apple. Instead, what's happening is Siri will automatically transcribe your request, as it already does. That transcript will be kept so that Apple can check the sort of questions that are being asked. What's not being kept are the voice recordings, which are obviously far more surveillance-y. That's going. It will not be kept by default, and that greatly reduces Apple's ability to improve Siri, which is why it's not the only change it's made. Number two, users will be able to opt in to sharing their recordings with Apple. The company said, we hope that many people will choose to help Siri get better. This is obviously better than putting the onus on the user to opt out, right? This is absolutely best in class, and I was stunned to see they are doing it. Opt in, so you having to actively make the choice to share your information with Apple, to share your recordings with Apple, is the only justifiable way to do this sort of improvement. But every company has been terrified of doing it because they're all terrified that no one will opt in, that people will, you know, freeride, they'll go, well, I want to use the service, but I don't want people listening to me to improve it. There are obviously ways that Apple can nudge the dial. I think it's extremely unlikely that this opt-in will be buried in a menu somewhere, and you won't be told about it, and you'll have to actively go to it. I think it will more likely be a big screen that will come up the first time you turn on your phone after updating it to iOS 13 that will say, do you want to send your information to Siri, these voice recordings will be listened to by people, yes, help improve Siri, and then in small gray text, no, don't. But it's still massively better than anything Apple's competition is doing. Amazon, currently the best in class, soon to be second best, does have a menu item buried three screens deep in settings that is not flagged anywhere else that simply says, help Alexa improve by sharing my information with Amazon, which defaults to on, and if you turn it off, then people won't be listening to you. I think that's a good setting, but that's the sort of thing that I expected Apple to do. This is much, much better. And finally, change number three, only Apple employees will be allowed to listen to those audio samples. The company previously outsourced the work to contracting firms, and over the past few weeks has ended those contracts, resulting in hundreds of job losses around the world. Does that mean your whistleblower is out of a job? It does. Apple is keen to insist that because they never directly employed the Siri contractors, they cannot be held liable for them losing their jobs. And that is the contracting firms that decided to lay them off, and that these contracting firms could, if they wanted, have kept them on the books and put them on some other work. That's pretty disingenuous to me. I think if you sign a contract with a company and that company then hires 300 people as a result of that contract, if you end the contract, you are responsible for those 300 job losses. At the same time, if people are employed doing work that shouldn't exist, the fact that that work shouldn't exist has to be a baseline in this discussion, and listening to secretly recorded audio snippets of people who didn't give their consent for it is not a job that should exist. So yeah, the whistleblower who told me about this originally has lost their job, as have the other five whistleblowers that I've spoken to since. And this kind of blunt solution that Apple has taken, does it even fix the problem? So is it much of a difference really if it's just Apple employees rather than contractors that's listening to your accidental recordings of sex and drug deals? Let's not be naive. There is one area where this is a major difference. Apple employees are far less likely to speak to the Guardian. I don't think that's a coincidence. I think one of the reasons for this change is that Apple has realized that what it had previously categorized as low skilled labor that could be farmed out is actually very sensitive work that should be carried out in-house, in part so that it can be held to the same standards of secrecy as everything else that Apple carries out in-house. More generally, this is a win for labor protections. There's no doubt about that. People being on the books as full-time employees is great, and people being off the books in tenuous employment is not. There will be less of this sort of work. If you make this opt-in rather than opt-out, if you keep the vast majority of information as transcripts rather than audio recordings, there will simply be fewer recordings to go through, which means that Apple can, on average, pay more per grader, which means that they can afford to start taking them in-house and offering them the perks and benefits that come with being an on-the-books Apple employee in Spain or Ireland or elsewhere. This got me thinking about another story that Alex wrote recently, about Facebook launching a clear history tool. The move is part of a wider set of tools covering off-Facebook activity. It was one of the many post-Cambridge Analytica pushes that Facebook did to try and clean up its image. The clear history tool lets you disconnect that history that Facebook has created for you from your personal Facebook profile. It will not actually clear your history. Facebook will still retain all that information. The company says that the way its tooling is built means that it can't really fully delete it. It will just assign it an identifier which is not linked with you and effectively not use it for any of the stuff that it had been using it for. It will still stay on its servers. It will still know that there was someone who visited these things and did this stuff, but it won't know it was you. It is better than what came before. It is not perfect, but it is improvement. These companies are acting. They are slowly grinding their gears and enabling things that are actively bad for them, that make their life harder, but that are needed to put meat on the claim that they care about privacy. So, as you said, tech companies are moving forwards and making small changes, but it does sometimes seem, and I'm sure that they would deny this and there's no way of proving it, that big tech often responds to scandals by making a kind of grandiose move or statement without necessarily fully addressing the problem that has been unearthed. Am I right to think that or am I wrong? I think you're right and I think the reason for it is that they don't sit down and think about what a term like privacy means. They don't sit down and think about what the relationship of consent to data processing should be. They sit down and think about what they should say it is. They sit down and think about what the legal requirement is. Maybe in some side offices, a few execs do sit down and go, we should hammer out a senior 2020 vision for what we're going to think of privacy in the next decade being, but that doesn't guide action. That's not how many companies work and the problem is that that means that they only ever act in two ways. One is reactive to scandals like you say and the other is what guides them generally, which is building the quote unquote best products, pushing the technology forward, creating things that their competitors can't. That's always going to guide their action and the problem is it's hard to view that in a way that guards people's privacy and it's very easy to view that in a way that pushes the boundaries of what you can exploit, what you can take and what you can survey. So that's just one more thing I can't trust in this world. Thanks Alex. But really, big thanks to Alex Hearn for joining me this week. There'll be a link to his work on this story on the Guardian website. Chips is produced by Danielle Stevens. I'm Jordan Erica Webber. Thanks for listening.