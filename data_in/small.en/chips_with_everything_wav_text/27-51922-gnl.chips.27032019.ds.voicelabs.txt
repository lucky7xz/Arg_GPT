 The Guardian. Hello there, Jordan Erica Webber here. I know you aren't used to hearing my voice on a Wednesday morning, but this week we're posting a special bonus episode to look at a particular type of technology that The Guardian has built and is currently testing. It's called The Guardian Briefing. Hello and welcome to The Guardian Briefing. Here are three of today's top headlines. Headline one. The team at Guardian Voice Labs is experimenting with generating an audio news summary by blending human and synthetic voices. It's designed for Google Assistant and based on existing Guardian journalism and curation. The idea is to capitalize on the text-to-speech technology on the Assistant platform and create a new way for people to digest the news of the day. Hi Jeremy, thanks for coming in. It's great to be here. To better explain exactly how the technology works, I invited the product manager for The Guardian Voice Lab, Jeremy Pennekuk, into the studio. This is a bonus episode of Chips with Everything. Where did the idea to have Guardian News for the Google Assistant come from? What's the kind of goal there? So news is one of the things that people say that they use on these platforms, especially on smart speakers, but it's also something that they don't say they value very much. So there's this huge gap between how often they are asking for news and the type of news experiences they're getting and how much perceived value they find in those things. So a lot of people, maybe about half of people, never do anything that is not on the tin on these devices. So they play music, they ask like general knowledge questions, what's the weather, set timers, and then what we're trying to figure out is like what is the right shape of news experience on these devices? Is it more sort of this passive ambient radio experience, or is it more something that you are really interacting with in a different conversational way? Yeah, I think my experience with Google Assistant is pretty much 50% setting timers, 50% asking for songs on Spotify, and only half of those ever work. Let's talk about the relationship with Google. Are they funding the project? How does the relationship work? What do they get out of it? Yeah, so I think the so it's part of a long partnership that the Guardian has with Google. We worked with them on a VR lab as well. And so what they're helping do is fund the R&D project, basically. And I think what they get out of it is better partnership with us. But I think they're also very interested in publishers figuring out what the shape of news experiences should be on these devices. They've built like amazing underlying technology, but it's sort of up to us to figure out what those content based experiences are going to be. So let's get to the technology itself, then how did you go about building this tech? So the technology is quite interesting, because it's very new. But it's not maybe as complicated as you might think, because a lot of this really heavy lifting of the machine learning and all of the AI are done by the platforms. So what we have to do is really within the constraints that they give us, figure out what is the right experience. And that is really the difficult part, because it's a totally different paradigm to a website or an app. What does it mean to talk to the Guardian? What does it mean to talk to a computer? Everyone needs help figuring it out, the sort of general literacy around that isn't very high. And there's not a lot of standards or expectations around what should be in a voice experience. So we're really trying to figure out how to combine what the platform does offer you and what it is really good at is figuring out what you said. And then we have to decide what to do with that information. And it's not so easy to figure out all of the things that someone might say and how to decide what to do based on that intent. So how did you use the tech then to get that result? So there's sort of two main things you can do in a voice first experience, which is you need to either provide some sort of audio that is recorded. So a rich audio, we call it should be something like you prerecord something and you play it when the computer says something specific or when the user asks for something. And then the other thing that these platforms offer is text to speech technology. And this is combined with a sort of a markup language called SSML and kind of think of it as HTML for sound. So it's kind of telling the computer what to say and how to say it. And there's all these interesting nerdy things around prosody, pitch, emphasis. So you can tune and tweak how the machine actually reads your text content, but you don't actually have to prerecord everything. And what we've tried to find in our last project was a mix of those two things because the text to speech robot voices, people don't automatically like them and they can get grating and difficult to listen to for long periods of time. But there has a huge amount of power because it's very dynamic. It can be very fast and up to date and it pulls from a huge variety of text sources. On the other hand, the natural audio of a human voice is just more pleasing. People connect with it in a different way. We can put things like music in and whatnot. So trying to combine those things is something we've been really interested in. And here's headline two. New Zealand gun laws. Jacinda Arden has said her country will introduce new firearms laws in the wake of Friday's terrorist attack on two mosques in Christchurch, but did not announce specifics. Do you think it'll affect the way that people take in their news if they hear it this way in this kind of blended human voice and synthetic voice combination? I think people actually already sort of know the shape of what this news that they want out of these things is. People say they want really brief things that they can control really easily or like really long passive experiences. I think the more difficult thing is when you can't tell the human from the robot. The Google-powered voices just got a lot better when they released these new WaveNet voices, which are actually powered by DeepMind neural network technology. And the leap forward is just incredible. So I think it's going to be really interesting, just the pace of that, where already it's quite difficult to tell sometimes, is that a human voice or not? And I think even beyond that, we might need to think about, well, what does a Guardian specific voice sound like? Do all voices on the machine sound the same? How can you actually differentiate between what one publisher and another sounds like? So I think there's a whole lot to unpack there when it comes to people's expectations around that and what they want to listen to generally. Yeah, do you think there are worrying implications for if you have a very realistic sounding robot voice that sounds like a human reading Guardian news? Say if something goes wrong with the code, there's a bug and it's reading an incorrect story and it's a voice that sounds human and it says it's from the Guardian. How do you deal with those kinds of issues? So one of the things that we do in product development is we decide to optimize around certain things. And what we're trying to optimize around in the project we're working on right now is really against these exact things that we call hard fails. So we have these different categorical systems to define, is this a good machine read? Is it a okay one or a bad one? And these bad ones, we're really trying to optimize around. We'd rather have something that was slightly awkward or maybe not as good as it could be rather than something that's incorrect. So I think for us, the onus is on us to set up an automated system that has these baked in editorial philosophies embedded in it rather than just try to set up whatever is the fastest or this easiest. So we're spending a lot of time combing through the output of those, this automatic pipeline and trying to figure out which ones work and which ones don't because we really want to make sure that that editorial judgment and curation is the real value of it. Don't go anywhere. We'll be right back. It's time to focus. I think ultimately that ideology is fading, but it will have a sting in the tail and we see that sometimes with these flare ups and violence. Today in Focus is the new daily podcast from The Guardian. Join me, Anushka Rastana, for the best stories from our journalists around the world. Subscribe now to Today in Focus from The Guardian. Welcome back to Tips With Everything. I'm Jordan Erica Webber. So when did you launch and what's the response been so far? So the latest thing we launched was called The Guardian Briefing and it's a sort of a news summary update that's less than two minutes and it combines, like we said, these human and synthetic voices together using these two different technologies. So it's been good so far, but discovery is one of the most difficult things on these platforms because people don't know what they can do with them and there's not an app store exactly. There's no icon on your screen that you just tap. So how to find people is really difficult and even with the megaphone of something like The Guardian, it's been so difficult to get people to know that you can do this thing. When they've discovered it, they've been really interested and we've actually tried to go and talk to people who already like things like The Morning Briefing, which is a source that we pull heavily from in terms of taking that journalism that's already being done and adapting it and repurposing it for this different platform. And so we think that those people are the most interesting to help us figure out, do people like it? How does it work? We've seen really a mixed response. Some people really didn't like the robot voice. Some people say, wow, this is great because I have some vision impairment and now I can listen to something. So it has really a lot of potential, but I think we're just cracking the surface of it. Have there been any glitches or technical difficulties that you weren't expecting? I think the most difficult thing is that these platforms are very new and we are used to a certain amount of feedback in terms of data and it's been very difficult to see what we're doing right or wrong. So we're still making a lot of guesses and assumptions that we have to figure out, whereas if we were making a website or an app, we would maybe have some more tangible evidence, some data to tell us whether we were doing a good job or not. So I think that's the most difficult thing is just insight into what our audience is doing. Are they liking it? So we're going to have to rely a lot more on qualitative evidence from people writing in and telling us how they're feeling about it. The other thing that we really is difficult is this idea of invocation. So invocation is how you open an app on one of these platforms where you have to tell Alexa or Google or whatever to launch this experience. And getting that exact phrasing right is really, really difficult. Even just having the or not the Guardian briefing can make a huge difference on whether the app actually opens or not. So what is the invocation for this? It's Hey Google, ask for the Guardian briefing. We're seeing more and more voice tech, it seems. And if you think about it, this is an example of humans interacting with a machine in a way that we previously would have only interacted with other humans. Do people actually want to talk to machines like this? Is there research out there that shows that people really want this tech? I think it's really interesting when you look at kids. When the iPad and things like that first came out, it was super obvious how a touchscreen was something really natural to people. You could see young kids who couldn't barely read would know how to swipe, tap and pinch and zoom and all of these things. Because they're intuitive gestures. I think that voice has the potential to be actually a more inclusive technology in some ways because the barriers to engaging with that are much lower. We've seen a lot of evidence that suggests elderly people are quite fond of this technology because maybe they never quite got up to speed on their laptop or their iPhone. But they can just talk to it. And it does simple things for them. So a lot of the early adopters have actually been really different from what you'd expect from the general tech people who are very into technology early. I think the other thing that's surprising is how much fun people have with this. There's some great research from Reuters about how social these devices are, especially the speakers. Because they sit often in the living room and are the centerpiece of conversations sometimes where you're having a conversation with a friend or a family member and you are sort of including the speaker or the AI in the conversation. I think that can be really fun and funny in a way that maybe even the device manufacturer didn't expect it to be. The idea of voice tech as inclusive is quite an interesting one because we've seen examples of people with strong accents maybe not being able to interact as well with voice tech or I guess people with speech impediments would probably have difficulties as well. Are we getting anywhere with that or is that just always going to be a problem? I think the pace at which these things are evolving is incredible. So I think that they will only get better. I think that there is still always the need to be more inclusive in all of these things. I think there's a huge potential for a whole next wave of people coming into the digital space that maybe have never really used a graphical interface. And to do that I think we have to put our full empathy hats on and really understand what are the challenges for somebody who may have a speech impediment or what if they're blind and this is the first real great interaction they've had that's not a screen reader or what if they are from a if they have a language barrier because they speak a slightly different dialect or something like that. And I think the systems are in place to have that evolution happen. I think we just have to continue to push to be inclusive in our designs of all those different people. How could this kind of thing affect the media landscape in general? So media organisations are trying different ways to entice the public to listen to their news. So how do you think this will affect the kind of news content that's produced if basically we end up in a position where we're all just asking our smart speakers for a roundup? I think maybe one of the biggest more interesting things in the long term is that it's not about smart speakers, it's about voice as this access point to the knowledge of artificial intelligence. So when you ask for queries like what's going on with Brexit or something else in the headlines, what happens to decide what bit of information you get back on that? How do you know that's good information? How does the publisher actually, how does the economic model work? Like you don't maybe get an impression. So if we just have a little explainer on our website that says something about what's next in Brexit, it may just read that one paragraph. But is it clear that it's The Guardian? Do we get any impressions for our advertising or do we get an opportunity to upsell membership? I think there's a huge future around where we just ask for information and where does that information come from? Yeah, so The Guardian briefing obviously will restrict people to only content that comes from The Guardian. What are the advantages and disadvantages of that compared to say people just asking for news generally and not knowing where it comes from? I think the defaults will be really important here. So most of these platforms have some way for the user to customize where their new sources are from. And I think that they have been very careful to try to include only reputable sources in those experiences. At the same time, most people don't change the defaults. So I think it's going to behoove a lot of users to try to figure out that they are, they might be in a filter bubble. So you go out and find additional new sources that are not just the standards that come as the defaults. But I also think that there's a lot more that third party developers can do to try to build experiences that are more diverse when it comes to where those new sources come from as well. There's something quite cool about this idea, I think, of waking up and asking your friendly smart speaker for the news. It's the kind of thing you see in science fiction. But is there a risk that it will just be a fad? Absolutely. I think we have to ask ourselves where on this hype curve we are, right? And if you're not familiar with the hype curve, it's this idea that people we get as a society get really excited about the prospect of new technology. And we invest a whole lot in it before we understand what it's really useful for. And then as we decide, okay, we've over invested, we've got too hyped about this, then this trough of disillusionment happens before the plateau of productivity. And I think we are somewhere on that hype curve as for this technology as well. It's the devices that this technology is enabled are selling faster than mobile phones did 10 years ago. So I think it's definitely not a fad. But I do think there will come a reckoning where the economics and the viability of building for this technology, especially for media companies, is still not clear. And we have to figure that out before we figure out what the long-term strategy is around that. Whether or not we all end up getting our news through our smart speakers in future, voice tech is big right now. And I'll be interested to see how its popularity affects the ways we access content, the kind of content we get, and the power the platform holders have. If you want to check out how The Guardian is using this tech, there'll be a link to The Guardian Voice Lab's blog post on the episode description on The Guardian website. Go and check it out. I'll be back on Monday at the usual time for Chips with Everything. For Chips with Everything, I'm Jordan Erika Webber. Thanks for listening.