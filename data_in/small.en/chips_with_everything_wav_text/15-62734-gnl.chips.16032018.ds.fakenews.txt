 The Guardian. People don't spread rumours anymore. They spread fake news. Fake news media. The fake media. Fake news. Fake news. Fake news. Fake news media. One of the most widely used phrases throughout the 2016 US presidential election, fake news was named Collins Dictionary's Word of the Year for 2017, and it's still going strong, leaving some people questioning the validity of every story that pops up on their social media feeds. Celebrities post inspirational quotes on Instagram, attributing them to someone called Anonymous. People use fake identities to spread images with a false framing, exploiting terror attacks to push a hateful agenda. Every time there's a big storm, someone confuses things by circulating a video from a previous storm. Politicians can apparently just protest fake news in the face of any accusation. Some of you probably find this trend quite irritating, but the spread of fake news can have devastating real world consequences. What we found was that some of the information was true or ended up being true, but a lot of it was misinformation. It was false. It was maybe malicious, maybe not malicious, maybe just errant false news. And so it was essentially misguiding us in terms of where it was safe to be or where it wasn't safe to be. And chances are you've played some part by clicking the share button or casually mentioning something you saw online without bothering to fact check. In early March 2018, a paper published in the journal Science describes how researchers found that on Twitter, lies spread six times quicker than truth. It also showed that we need to take a good hard look at ourselves as it is humans that are more likely to spread fake news and we can't just blame the robots. But why are we so quick to believe fake news over the truth? Maybe false news is simply more novel than the truth and is surprising and therefore people share it. Will this problem just get worse in the future? And is there some technological way to detect the presence of fake news before we humans start to spread it? And what we are trying to do is in some way catching up on this development and trying to come up with tools that are able to detect these kind of manipulations. I'm Jordan Erica Webber and this is Chips with Everything. This is Sinan Aral. I'm the David Austin Professor of Management at the MIT Sloan School of Management. Professor Sinan Aral is one of the three researchers behind the paper The Spread of True and False News Online. Their study was pretty unique so I started off by asking Professor Aral why it was set up. And I believe that social media is a turning point in human communication and human coordination. I believe that it is having dramatic effects on our democracies, our politics, even our health. So these were the broad reasons why we're interested in the information health of social media. But a more acute reason was the Boston Marathon bombing which essentially shut down the MIT campus. There was really no understanding of where the terrorists were hiding. There was no official word as to what was safe and what wasn't in terms of where to go. We were just told to stay indoors. You may or may not know that an MIT police officer was shot and killed during that manhunt for the terrorists who were hiding out in various neighborhoods in Boston. And so we naturally turned to Twitter to try and understand what was going on. At that time, Sarush Vasugi, who is the lead author of this study, was doing a PhD thesis on the spread of information in social media on Twitter. And when we were looking at Twitter to see what was the information of the moment, what can we find out about what's going on out there as we're locked down in our buildings. And what we found was that some of the information was true or ended up being true, but a lot of it was misinformation. It was false. It was maybe malicious, maybe not malicious, maybe just errant false news. And so it was essentially misguiding us in terms of where it was safe to be or where it wasn't safe to be. And at that moment, Sarush met with his advisor, Deb Roy, who's the other co-author of this study. I was also on Sarush's PhD committee. And he decided at that moment to change the direction of his thesis to study the veracity of information spreading on Twitter. And his thesis was about how to predict whether a story would be true or false. And once we had collected a whole corpus of data about true and false stories that would aid in calibrating that algorithm, I suggested to Sarush that, well, maybe we could say something even more broadly about the spread of truth and falsity misinformation online if we were to do a systematic large scale study of it. And that's exactly what we've done here. So the data in the study, you said, spans the life of Twitter. So 11 years from 2006 to 2017. Is that right? It ends on the 31st of December, 2016. But yes, from the first tweet to December 31st, 2016. So in the time that these tweets represent, did you see a change in the spread of false news over that time? Or was it kind of a gradual thing? So we did see in our data an increase in false stories over time. That increase came in fits and spurts. So we see spikes of false news occurring during the 2012 and 2016 US presidential elections. But overall, there's a general positive trend towards more false news over time. However, and this is a very big caveat, it is not clear that that means that false news is increasing on Twitter or in social media. It could just be that fact checking is increasing over that period. And so more stories are being verified each year over that time period. It's difficult for us to tell which of those explanations is true, or in fact, whether false news is increasing per se. Okay. So what would you say are the key pieces of information that came out of the study? So the main result is that false news spreads farther, faster, deeper, and more broadly than the truth in all categories of information that we studied. That's by an order of magnitude difference. False political news in our data traveled farther, faster, deeper, and more broadly than any other category of false news. And we were really interested in why this was happening. So we looked first at some potentially obvious explanations. So we thought, well, maybe characteristics of the spreaders of false news can explain this. For instance, maybe people who spread false news simply have more followers, or follow more people, or tweet more often, or are more often verified, or have been on Twitter longer. In each of those cases, the opposite was true. So false news was spreading farther, faster, deeper, and more broadly than the truth despite these characteristics not because of them. Then we looked at another explanation, which we call the novelty hypothesis, which is that maybe false news is simply more novel than the truth and is surprising. And therefore, people share it. And what we found was that previous research showed that novel information was indeed more valuable and also gave people social status when they shared it. And when we measured the novelty of false news compared to true news, we found that false news was much more novel than the truth. And it was, in fact, 70% more likely to be retweeted than the truth by human beings. The final thing we did was we looked at whether bots, automated robots, could explain the spread of false news. We used two state-of-the-art bot detection algorithms. We removed the bots from the data. We put the bots back into the data, and we conducted the same analysis both with and without bots. And what we found was that indeed, bots were accelerating the spread of false news online in our data. But they were accelerating the spread of true news at approximately the same rate, which means that bots cannot explain this massive difference between the spread of false news on one hand and the spread of true news on the other. Unfortunately, human beings are a lot more responsible for that than we originally thought. Did anything you learned particularly surprise you? Yeah, I think the two things that surprised me the most were the sheer magnitude of the difference between the spread of true and false news. And the other thing that surprised me was the bot result, which was contrary to what we had been hearing a lot of in the media and even in congressional testimony here in the United States in front of the House and Senate Intelligence Committees that bots were a big reason for the spread of false news. So, unfortunately, it seems that we humans are really to blame for the spread of fake news. As Professor Aral explains, this is worrying for a number of reasons. The spread of false news can lead to misallocation of resources during terrorist attacks. As we discussed with the Boston Marathon bombing, law enforcement can be sent to the wrong places if they use Twitter to decide what to do. It could lead to misallocation of resources during natural disasters. There's a story that a false tweet that claimed that Barack Obama was injured in an explosion wiped out $130 billion worth of equity value in a single day. I also believe that we are having an important conversation these days in society about the role of false news and the spread of false news for our democracies and our politics. So, when we think about all of that together, it says to me that these issues are real and they're very important and it's not just a matter of a social media nuisance. I think we need to, as a society, think clearly about how we can learn more about this problem and what we can do about it. Do you have any ideas for solutions? Yeah, I've got a couple of ideas. So, one example would be labeling. When you go to the grocery store and buy your food, it's extensively labeled. But we don't get that kind of information when we consume news. We don't know how often does this source put out true or false information? What's the likelihood that this story is true or false? We know none of that. We just know what's the information in the news. So, if we were to label news, perhaps people would make different judgments about what to spread and what not to spread. Another example would be incentives. So, the social media advertising ecosystem incentivizes the spread of false news because people get paid in advertising dollars the more people they get to read a certain piece of content. Our work shows that false news travels farther, faster, deeper, and more broadly than the truth. So, there's an economic incentive to spread false news and be paid for it. This might be counteracted by disincentives. So, if platforms were to reduce the reach of sources that are spreading false news or the reach of false stories, then there would be less of an economic incentive to produce them in the first place. You can also think of technological solutions, tweaks to the algorithms that reduce the spread of false news compared to the truth. I think there are a number of very difficult challenges ahead of us in stemming the tide of false news. So, Professor Aral suggests some possible ways to stop the spread of fake news. But is there some way to figure out what's real and what's fake before humans get a chance to fall for it? What we actually try to do is we try to make the machines think like a human or like a journalist how would he approach the task of verifying content. We'll talk about that more after this short break. Hi, Daniel Glaser here. Our latest episode of A Neuroscientist Explains looks back to our evolutionary past and asks, where did social conformity come from? What we're interested in explaining here is the variability we see out there and in the world before us. The differences between species and between populations within a species. How can we explain how that arose? This mode of life is called by the Earth's social scientists the culture of the group. Is does not mean all. Just because you can explain why something is the case doesn't mean you have to behave that way. But on the other hand, understanding why it has that kind of background may allow you to change it by changing the context. To have a listen, head over to theguardian.com forward slash podcasts or search A Neuroscientist Explains on your favourite podcast app. Welcome back to Chips with Everything. I'm Jordan Erica Webber. Before the break, we spoke to Sinan Aral, an MIT professor and one of the scientists behind a study on how quickly real and fake news spreads on Twitter. He explained that although there are a select few humans and robots that create fake content, in most cases it is humans who spread it. And you're holding it just a little bit away from your face? Indeed, yeah. Paul Levi is co-founder and CEO of AdVerify. An artificial intelligence company that aims to fight fake news. Okay, so how exactly does it fight fake news then? Good question. So what we actually try to do is we try to make the machines think like a human or like a journalist. How would he approach the task of verifying content? How do you make a machine think like a journalist? So we have this checklist that we are going through. So we're starting from the source of the content. We're looking into the issues like whether this is a story by trusted news agents. We're checking if it's coming from a non-SATA website like The Onion. We're checking if there's issues of political bias. And we're also checking things like we have been seeing a lot of cases that fake news purveyors are impersonating known brands with their URLs or with some branding. So that's also something we're looking into. The next step, we're looking into the story itself, trying to see if there's some references to that from legitimate news outlets. And in the next step, we're also trying to see if this story was already debunked by one of the fact-checking organizations. And they have aggregated archives of fake stories that they've already debunked. So we can use that to flag stories if they are being recycled. And then the final layer, what we're doing is we're using machine learning and natural language processing, which is our expertise in order to find some patterns that could help us to distinguish between fake stories and ones that are more likely to come from high-quality journalist sources. Okay, so what kind of patterns does the algorithm look for then? We're looking for things related to the topics that the story is talking about. For example, if it's political, if it's talking about some public figures, or looking generally into the keywords that are mentioned in the story. And we're also looking into what we call psycho-linguistic use. So trying to assess the sentiment of the story and also the writing style. So we're seeing that fake stories are using more expressive language, using, for example, more adverbs and adjectives expressing more sentiment than when you compare it to high-quality journalist content. Do you think that that level of sentiment and emotion in fake news stories explains why they're shared so widely? Of course, I think this is actually what helps us to identify the story. So I think it helps to understand what are the motives of the fake news pervert. So it's not like people wake up in the morning, decide to create a website and put some fabricated stories there. I think the reasons they're doing that is because they think this costs nothing. And they're saying, if I will write stories that are without outlandish headlines about juicy topics, I will get a lot of traffic. And then I could monetize that using advertising. So I think this is what they are trying to accomplish, trying to trigger this reaction, these emotions that make us react to these stories and press them forward. And yeah, this is also what helps us to identify these stories compared to the high-quality journalism content. So you said that one of the first steps on your checklist is figuring out whether or not the story is coming from a trusted news agency. Who decides which news sources are trustworthy? We are working also with a team of journalists. We're helping with curating the list of trusted sources and also looking into issues of political bias and different types of fake news, whether it's clickbait, set up, propaganda, AIDS speech, et cetera. Was there some particular piece of fake news or some moment that inspired you to create this company and make this product? It actually, it's an interesting story. It started as a game. So I was playing around with deep networks. And if you've seen examples of taking content like Obama speeches or Shakespeare writing, trying to make the machine generate a new story or a new text automatically. So I stumbled upon a data set of fake news. I was thinking it could be funny if the machine would learn how to generate fake news. So I created this app which would show stories, some of them real, some of them fake, and you need to guess. And I sent it to some friends and it actually worked. So it was very hard to distinguish the real ones from the fake ones. And then I was thinking if we are able to generate them, then therefore we might be able to detect them and distinguish them from the real stories. So what makes humans so bad at detecting fake news then, like in the case of this game that you made, and how does your algorithm compensate for that? Yeah, so I think what makes human prone to fall into some of the manipulation that's coming from the fake news perverse is that these stories are trying to bypass your rational thinking and triggering emotions. And when you get into this state, the emotional state, you think less rationally. And this is when you're open to more manipulation and persuasion. And what our artificial intelligence is trying to do, so it's not replacing humans, it's more about helping humans, providing them with insights about the content they're really doing. Do you feel like there's a sense in which you and other companies like yours are just playing catch up? Yeah, exactly. So it's some sort of an arms race. And we're seeing with the past few years, very large advancements in artificial intelligence and deep learning. So this makes also tools that help to create realistic content more accessible. So we've seen examples of algorithms that are able to generate speeches. For example, there was some algorithm that took Obama's speeches and you could enter whatever text that you want, and it would make it look completely realistic as well. Obama was saying that. So put some tools in the arsenal of fake news perverts. And what we are trying to do is in some way catching up on this development and trying to come up with tools that are able to detect these kind of manipulations. So it's possible that human beings let our emotions get the better of us. As Orr explains, we sometimes need the help of a rational artificial intelligence, especially if those creating fake news are using AI too. But we can't rely entirely on the power of machine learning. Humans need to work with these algorithms to decide what sources are trustworthy and, ultimately, to have the final say. My thanks this week to Professor Sinan Aral of MIT and Orr Levi of AdVerify. You can find a link to both AdVerify and Professor Aral's study, The Spread of True and False News Online, in the description for this episode on The Guardian website. I also want to hear from you. If you have any questions or feedback on the show, and if you have any ideas for cool digital stories that we should cover in future episodes, email us at podcasts at theguardian.com. I'm Jordan Erika Webber, see you next time.