 The Guardian Have you ever been thwarted by an automated system? Just this morning, I tried to log into a website I haven't used in years so that I could delete my account and stop getting newsletters, but the password wouldn't work. So naturally I clicked on the link to reset the password, followed the instructions in the automated email, and it just failed, with no clue to how I could solve the problem. You've probably had similar experiences. Maybe you've gone so far as to phone a helpline, only to be met with an automated voice message telling you to go through the steps you've already tried multiple times. Now imagine that the system you're trying to navigate is related to your ability to claim benefits, with the risk that it will flag you as a potential false claimant, and the result of this would be that the money you need to pay your rent or buy food wouldn't reach your bank account on time. The Department for Work and Pensions, our big welfare giant in the UK, the government's welfare wing, is investing more and more in automation of its flagship benefit, Universal Credit, and has stepped up its attempts to use more robots and more machine learning to process claims, to evaluate claims, and to interact with claimants. Last week, the UN Special Rapporteur on Extreme Poverty and Human Rights, Philip Olston, wrote a damning report in which he expressed concerns about the emergence of the digital welfare state. The computer systems that the government are building are being tasked with making evaluations and decisions to some extent about claims and about people inside the black box. The same week that report was presented at the General Assembly, a special Guardian series revealed that the Department for Work and Pensions is pushing ahead with the development of machine learning in the benefits system, and that one in three councils are already using computer algorithms to help make decisions about welfare. Governments in favour of this kind of digitisation argue that getting machines to take on work previously done by humans will save time and money, and would reduce the level of welfare fraud. But people like Philip Olston warn that this so-called digital transformation will only embolden governments to quote, automate, predict, identify, surveil, detect, target, and punish. I'm Jordan Erica Webber, and this week we look at the consequences of living in a digital welfare state. This is Chips with Everything. My name's Robert Booth, I'm the social affairs correspondent for Guardian. Now we're going to be talking to you about automated welfare in the UK specifically, but this story was part of a bigger series by the Guardian, is that right? That's right, yeah. We decided that we wanted to look at this subject internationally because it's happening in all sorts of different countries. So we're looking at it in the UK, but also North America, the US, Australia, and India as well. Rob recently wrote about the various investments that the Department of Work and Pensions has made into what is being called an intelligent automation garage. It kind of made me smile when I saw what they called it, because it sort of made me think of the beanbag culture of advertising agencies or Silicon Valley, or it's trying to sound funky, isn't it, to attract people to get involved. Pool tables. Exactly. And in fact, a lot of the job adverts for jobs at the intelligent automation garage do talk about the proximity of the nearest gym and how you're close to great facilities. They're trying to sell it as a great place to work, but basically what it is is a unit where they're developing robotics for use in the welfare state, machine learning, algorithms, AI, and they have it based across a couple of offices. It's nothing like Silicon Valley, unfortunately. One's on the outskirts of Newcastle in a very drab, grey building, and the other branch is in Manchester. So they have that, but they also have something called the Innovation Dojo, and they're obviously trying to cultivate this innovation culture there. What is the reasoning behind moving to a more robotic and less human-led system? Well, ostensibly, the government say, and DWP Digital say, that they're setting out to improve the efficiency of the system, to make it work better for the users of the system, that is the claimants, to use robotics to do some of the tasks that don't have to be done by human beings, thereby freeing up human beings for what they talk about as a kind of more important face-to-face stuff. But clearly, there are other benefits to it as well, which is that it would allow potentially the government to make tweaks to the system by changing the algorithms, if they needed to change the categories by which you actually can get welfare, whether you actually stand to receive welfare or not, and also just simply to save money. They've talked about, I think, 350 million quid's worth of savings in one of the job adverts are talking about making from some of these efficiencies. So there's obviously a kind of cultural drive, isn't there, to use more technology in all sorts of parts of life? And so there's a logic to it. It's there, let's use it. And they're saying that it's going to improve the experience for people, but the existing digital technology that there is is improving the experience for people sometimes, but in lots of cases, is having quite a devastating impact on people's lives. Obviously, the DWP is hoping to save money through this, but implementation is actually pretty expensive, right? Well, insofar as they have to let quite substantial contracts to consultants and IT companies, yes. I mean, I think what they're trying to do, the impression I've got is that they're trying to build up as much in-house capacity now as they've got, and their strategy is to do that because they're aware of that being potentially inefficient. But yeah, I mean, the welfare system is a huge super tanker in terms of the amount that we spend on it as taxpayers. So the sums are always going to be big when you're dealing with 20 million claimants and reconfiguring and redesigning computer systems to handle all of that. It's gargantuan scale, really. And which companies have been contracted for this work? There's been lots of multinationals involved. There's been IBM, Tata, Consultancy, Capgemini. And it's also working with a company called WePath, which is led by, it's a New York-based firm, it's led by this guy called Daniel Dines, who's Forbes magazine, called him the world's first bot billionaire, who's had a vision. And his vision is, I want a robot for every person. You say in your article that the DWP is building a, quote, virtual workforce to take on some of the jobs that are currently done by humans. So what kinds of jobs would those be? They're not entirely clear about that. They are trying to say that these are not key decision jobs about what happens with someone's welfare claim. They're more like the job of processing the claim, working out whether something needs to be double checked, trying to judge whether someone might be trying to pull the wool over their eyes and whether the claim actually needs to be rechecked or whether it might be fraudulent. So there's all these kind of sort of behind the scenes things that are going on that they think that the robots can take over. That raises the issue of how the DWP plans to collect the data that would feed the algorithm in the first place. One of the difficulties with this project was the lack of transparency around some of this. So when we asked the DWP to give us information about where they're pulling in data from, in other words, what data is the other robots, if you like, processing when they're evaluating people's claims, they wouldn't say. And likewise, they wouldn't supply information about kind of how the algorithms configured a sort of plain English version of what the algorithm is intending to do and the priorities that it's trying to make. So that unfortunately remains opaque. And the reason they say that they won't provide that information is that it would, in their view, enable fraudsters to game the system. So they say they have to keep this side of things under lock and key for the meantime. In the UK, automating the welfare system isn't just a future goal. In some places, it's already in effect. After the break, we'll look at how local councils have already run into system failures and why there's a lot more at risk than just a robot making a mistake. Welcome back to Tips With Everything. I'm Jordan Erica Webber. This week I'm looking at the consequences of the UK becoming a so-called digital welfare state. Before the break, The Guardian's social affairs correspondent Rob Booth told me about the UK government's latest investments into automation in the benefits system. But some local authorities have already started rolling out these kinds of systems, and the results are far from perfect. No, that's right. A colleague of mine also working on the project, Sarah Marsh, had a look at how local authorities have been embracing this kind of kit as they are squeezed, their budgets are squeezed, they're looking at ways of trying to save money and make quicker judgments on things they have to do. We've been using algorithmic software to assess the validity of claims for things like housing benefits and also council tax benefits, which is obviously things that millions of people claim. And that's been going on all over the country. 140 councils out of the 408 investigated have invested in software contracts that can cost millions of pounds, more than double the amount previously estimated. Perhaps more interestingly, to try and understand more about families and which families are likely to need support or which families might even be a danger to children. So there's a number of councils have been looking at ways of trying to predict, if you like, which, you know, where child abuse might take place in the future. That kind of thing sounds like a huge decision, especially for a machine to make. Have they provided any information for why they're not letting humans make those decisions and why they think machines can do it instead? So what they're using the machine for in those instances is to take a whole load of data about those families and like 40 different data things like school attendance, police records, health records, all sorts of things that you would put together and compare that profile to the profile of people who've previously abused children in those settings. So it's a kind of indicator that that family might be going down the same path. So they say that they're not making a decision. The computer isn't making a decision. The computer is providing a tip, if you like, hey, you should look in this area. You should maybe tell the social workers or the family liaison officers about that family. Maybe they should check in and find out what's going on. And then the human takes over at that point and follows it through. In instances where machines then have been used to help make these decisions and you get a human being coming to your door, if a person feels like that decision has been made wrongly, is there any way for them to complain about it or does the use of the machine limit that ability for them? In this instance of the child abuse prevention that we're talking about, the council would say that they're using it as intelligence. So they're within their rights to find out information about families and then to go and check up on families. So that would be their defense. Now, if you felt that someone had sort of gone too far, one of the things you could do if you wanted to find out what had happened here and how come someone's shown up in your door is you could ask using a subject access request under data protection laws the council to reveal the data that they use to get to you. But I don't think there's much else beyond that that could go on. Some councils have already abandoned contracts with tech companies after reports of errors with the systems. The same sort of technology has been used, as I said, to verify whether people are likely to be trying to trick the system when they're going for benefit claims, housing benefit and council tax. And one of those cases was in North Tyneside where they were using it. And they found that the data was actually pointing them to completely the wrong people. And it was identifying families as high risk of defrauding the system when in fact they were low risk. And the result of that was that benefits were being haulsed and claims were not being processed. So there was a real world impact of that. And this is one of the things that dealing with the consequences of it, we're just in the foothills really, we don't really know. There's no kind of methodology of coming back and dealing with this stuff because so much of it is buried inside these computers and many of the council officers don't necessarily understand what's going on either. A spokesperson for the Local Government Association, which represents local authorities, said, quote, good use of data can be hugely beneficial in helping councils make services more targeted and effective. But it is important to note that data is only ever used to inform decisions and not make decisions for councils. But concerns have been raised about the effects on people who need these services. I suspect what happens is that there's no kind of accusation. It's more that just things mysteriously don't happen. And then where do you go? There's a delay and you're not sure why. You know, it's that classic computer says no thing, isn't it? It's kind of this sort of frustration that builds up from not really being able to interrogate it. And then if you ask a human being about it, they can maybe look into the computer and find it, but maybe not as well. Then there's the broader issue of privacy and surveillance. As we mentioned at the start of the show, the UN Special Rapporteur on extreme poverty and human rights, Philip Olston, wrote a report imploring people to avoid stumbling zombie-like into a digital welfare dystopia. Philip Olston is a human rights lawyer and he is concerned that the rise of digital decision making or computerized decision making in welfare and the most vulnerable people around the planet is incompatible essentially with human rights. It's incompatible with the idea that people should be able to rely on each other rather than some sort of third party machine. And he's not alone actually in feeling that because as he points out in his report, the British Prime Minister Boris Johnson spoke on this very subject late last month at the United Nations as well. A future Alexa will pretend to take orders, but this Alexa will be watching you, clucking her tongue and stamping her foot. Your mattress will monitor your nightmares. Your fridge will beep for more cheese. Your front door will sweep wide the moment you approach like some... Boris Johnson is ultimately in charge of the welfare system that we've just been talking about and he asks the question, are we doomed? To a cold and heartless future in which computer says yes or computer says no with the grim finality of an emperor in the arena. And he talks about asking how do we know that the machines have not been insidiously programmed to fool us or even to cheat us? And he concludes by saying digital authoritarianism is not alas the stuff of dystopian fantasy, but of an emerging reality. So he's really aware of the dangers of this, talks about it as a sort of dark thundercloud looming over society. And yet maybe there is some of that going on on his own watch. One of the issues is that these systems are created not by government officials, but by big tech and that these companies have been notoriously slow to incorporate adequate privacy protections in their technologies. Elston writes, most governments have stopped short of requiring big tech companies to abide by human rights standards and companies often operate in a virtually human rights free zone. How does that affect the kinds of systems that governments end up using? I mean, in this case, we're talking about big companies having to be mediated by the government. They're just, they're not building their own welfare system. They're supplying services to the UK, in this case, the UK government's welfare system. And so it has its own standards that it will try to keep. So there is an extra layer of protection in there, I think. But the difficulty that we've seen, certainly with some of the local authorities and having spoken to people who have been working with local authorities and these tech companies is that there are there aren't that many people in the local authorities who quite understand what they're buying and they are being promised stuff and taking it because they need it, but they might not necessarily understand what's under the bonnet. There's another element to this, of course, which is sort of unanswered question, which is when you get these big third party private companies involved in processing the data that comes from the state about people's health or welfare, education and so on, we need to be assured that there isn't a back door through which that data can then flow out into those organizations. So whether it's a credit rating agency or something else, are they then able to use that data and access that data in a way which we haven't anticipated? And the civil servants in charge of this are going to have to be sharp on that because otherwise you have this, they talk about having this data lake, but if the data lake starts kind of washing into the private sector, it's quite a dangerous prospect. In response to the criticisms, a DWP spokesperson said that one of the benefits of automating this system is that it's quote, freeing up colleagues time so they can support the people who need it most. Did they give any indication of who they consider the people who need it most? And will this protect people who really need to talk to a human being from just being left to the bots? Well, I mean, no, they didn't give specific examples of that, but you can see the logic of what they're talking about there. I mean, the automation of mundane, repetitive tasks is clearly something that most people would welcome. That's one of the great benefits, isn't it, of technology? And it speeds our lives up. So I think we have to give them the benefit of the doubt on that. That is something that is a dividend from this. The question of whether it means that people who need human interaction most will get it. Well, for a long time, it's been getting harder and harder to get someone to pick up the phone or you're having shorter and shorter appointments if you're at the job center. So the extent of human interaction has been diminishing year on year already. And it would be nice to think that this would allow a flowering of human interaction and we'd be spending more time talking to each other and caring about each other. But the experience so far of digitization has not been that. These arguments over the digital welfare state are part of a bigger question that experts have debated for years now. Which jobs can be easily automated and which are best left to humans? I think this is probably less about automating jobs and the robots taking away jobs and more about the state having a more sophisticated digital portrait, if you like, of citizens and using all the data at its fingertips to build up that picture and then to make decisions based around that, especially when you get into difficulties and you need welfare or you need help. I think that there's a big question about consent around that. Do we want our personalities or lives to be kind of drawn in that way on databases and for council officials to have at their fingertips or government officials to have at their fingertips a snapshot of who you are according to that computer system that they've developed or bought off the shelf? It's not the same as sitting in front of someone. 80% of communication is non-verbal or whatever they say, getting this sense of what's going on for someone really. I think it's a little bit unnerving. I met a woman on the train actually this morning who I was talking to about this, this older woman who said she used to work for a local authority. She said that this system has been used for decades called ACORN that associates your postcode with a particular kind of economic status. Is this just a continuation of that kind of thing? Well that's very simple bit of data, isn't it? If you live in that area then you're likely to be that sort of person. This is much more... I mean, so in Bristol for example, the council there, they're splicing together dozens of pieces of data about each individual and creating a kind of three-dimensional what seems... This is the thing, isn't it? It seems like a three-dimensional picture of a person, but it's not actually. It's just certain facts and figures. The other thing about it is those facts and figures might be wrong. This stuff is only as good as the data you put in and the sort of shakiness of the data and the quality of the data when it's such huge data sets has got to be questionable as well. We're going to be here for a bumpy ride or stormy clouds as Boris puts it. Do you think that the DWP and other governments planning to roll out these kinds of systems more will respond to criticisms like those found in the UN report or will we just have to wait and see whether this automation works? I think that that's a really good question because having a debate about this and then defending it and other people criticizing it is difficult because it's so complicated. It comes back to what we were talking about at the beginning. The technology being used here is quite hard to understand for laypeople and whether it's being used for the right purposes or the wrong purposes is also complicated, sort of ethical and moral discussion. So I suspect that this is not going to be something that kind of erupts into sort of national debate and about which there will be great clarity. I think that what will happen is it will keep developing slowly and surely on a trial and error basis and there will be some benefits to it, there will be lots of drawbacks but it's just going to be our new reality. So I'm sure this won't be the last time that we'll talk about this and hear about these systems. Thank you very much for talking to me today. Thank you. Huge thanks to Rob Booth for coming in to chat to me this week. You can read the rest of the Guardian's Automating Poverty series on the Guardian website. And thanks so much to the woman I met on the train this morning who chatted to me about local authorities. As always, you can send your thoughts to chipspodcastattheguardian.com and if you have any spare time, make sure to rate and review the show wherever you're listening. Chips is produced by Danielle Stephens. I'm Jordan Erica Webber. Thanks for listening.