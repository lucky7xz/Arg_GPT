 The Guardian. Humans are hardwired to recognise patterns. This ability, honed over millennia of evolution, underpins our understanding of everything around us, from pictures and the written word, to oncoming traffic when we cross the street. Doing this kind of problem-solving and calculation is often second nature to humans, sometimes left to muscle memory or even carried out while performing another task. We implement these simple and complex processes hundreds or thousands of times a day. But you might be surprised by what we can call them. Algorithms. Yeah, they're in our courtrooms, they're in our hospitals, they're in our schools, they're everywhere. And I think that we're sort of entering an era where you can't just say, oh, I don't like the word algorithm, not getting involved. While once an academic term used by mathematicians and scientists, in today's quickly evolving, technology-driven society, the word algorithm has entered the general lexicon. We read and hear news stories about algorithms on devices that use them in a variety of ways. They're an integral and increasingly pervasive part of our existence in the modern world. But are they becoming too pervasive? And could they ever truly capture what it means to be human? I'm Jordan Erika Webber, and this is Chips With Everything. When you need to learn more about digital reductionism, you'll want to turn to someone who spends their time dealing not with human beings, but with numbers. My name is Hannah Fry. I am a mathematician at UCL, and I have just written a book called Hello World. Hannah joined our producer Max in the studio for a chat, where he started with the obvious first question. So we start off nice and simple. When people talk about algorithms, firstly, what are they talking about? And very, very briefly, what can they achieve? Okay. If there's one thing I've learned recently, it's that a lot of people really hate the word algorithm. I think that probably about 85% of people just want to immediately gout their own eyes when they hear the word. And I think that's quite understandable, actually, because the word algorithm doesn't tend to convey much meaning. And the reason for that is that it's this really broad term. It's this gigantic umbrella term under which loads of different stuff sit. So officially, all an algorithm is is a series of instructions. So something that takes you from some input via some logical steps through to some output. And that's kind of it. So in theory, when you stop someone and ask them for directions, and they say, go down there and turn left, they are giving you an algorithm. But the way that people use the word, it tends to mean the instructions that you give to a computer. So in that case, the input that you're giving the computer tends to be your data. And the output tends to be some kind of decision that the computer can make on its own. And in terms of what they can do, there's a few different things. Lots of algorithms are designed to just prioritise lists, which sounds incredibly mundane, but actually is a really powerful thing. Because if you think about it, Google search algorithm, all that's doing is prioritising the entire internet to deliver you a single list that matches your query. Or Netflix recommendation engine is prioritising their films. Even in some ways, the OK Cupid algorithm is prioritising all the people that you could want to date and delivering you a list of people that you want. There are other algorithms that will classify things. So we'll tell you whether a picture contains a dog or a cat, or we'll classify you as someone who wants to be served up ads for slippers online, or even we'll classify whether an individual, a defendant, is likely to go on to commit another crime in future. So the task they do on one level seem quite simple, but the power that they can have actually is pretty much endless. And the sort of part of that power has come from, like you say, algorithms have been around, well, arguably for hundreds of thousands, ever since we could talk, right? But part of the thing that makes it different, and I don't know if we want to come up with a word so that people who hate the word algorithms won't squint at this, but part of the difference is the amount of data we now have available. So very kind of briefly, that interplay between data and algorithms, I'm going to say algorithms a lot here. Yeah, that's fine. I know exactly. The word algorithm is almost unavoidable, unfortunately. What's that relationship and why, you know, why is it that they've somehow made these things more powerful? So the thing that's really changed in recent years is that it's not just that you have some clever mathematical tricks that you can apply in theory anymore, it's that we have this unbelievable wealth of data on who we are as people, on what we like, and on how we behave. And when you combine that with algorithms, you can make predictions about, you know, what we're going to buy, how we're going to vote, even what we're going to go on to do in future. And it's that combination really of the data on who we are, as well as these new kind of swanky algorithms that really put so much power in the hands of these machines. We've talked before about some of the ways that this power can produce alarming results, like when YouTube's recommendation system serves up seriously disturbing videos to young kids. But it's not all dark and scary. As Hannah explains, some fields, like medicine, might be able to use algorithms to save lives. So back in 2012, there was this big advancement where people realised that you could train an algorithm to classify a picture, right? And the original version of this was classifying the difference between a picture of a dog and a picture of a cat. But a little bit later on, people realised that this algorithm doesn't care what it's looking at, it doesn't really care whether it's distinguishing between cats and dogs. It can be distinguishing between tumours and perfectly normal tissue. So you can essentially take this algorithm, twist it a little bit, kind of shake it up a little bit, and take it out of its original context and then start applying it in medicine to help doctors triage potential cancer patients much more effectively. And even in some cases, you know, to diagnose things like problems with your eyes, but causes of blindness, that kind of thing. And this is just, I think, one of the really positive examples of how these same ideas that feel so oppressive in other situations can end up being of real potential for social good elsewhere. Yeah, and it's a nice example as well because, and again, it's another kind of, you have these common threads that run through the book and another one of them is this idea of, you know, a lot of the power of these things, whether for good or potentially for bad, comes from the interaction between the machine, say, and the human. So within that example, it's not just these algorithms coming up with this patient has cancer, this patient doesn't. There's a kind of relationship between them and the human doctors, right? Yeah, because if you just hand over control and power to a machine and say, off you go, diagnose all of these patients, then there's all kinds of questions and problems that come along with that because the machine isn't going to be perfect, right? The machine is going to make mistakes. And if you put flawed machines in a position of power, you have to think very, very carefully about what happens when something goes wrong. But one of the really positive things I think that's happening in medicine is that people are really embracing the fact that these machines will have flaws and are acknowledging the fact that we humans also have flaws in that we often, you know, over trust machines. We see them as kind of this easy way of absolving ourselves with responsibility. So they're really creating this partnership between human and machine and thinking of that at every step of the process of what are the potential pitfalls if you team up a human with a machine? I mean, we've spoken about how I really enjoyed your book because it wasn't, you know, all doom and gloom, but I think it would be inappropriate to paint it as too rosy. So within that chapter on medicine, you talk about these kind of systems being used for prediction as well. And so I think you mentioned a couple of companies like 23andMe and the kind of the issues that might come up around privacy. Okay, one of the big problems, right, if you are trying to get an algorithm to diagnose individuals or to even understand our own, you know, our own bodies better, you need a hell of a lot of data, a hell of a lot of data. You need hundreds and hundreds of thousands of records of individual people of details about their bodies and details about their condition. And that data is really hard to get hold of. So at the moment, the NHS is the biggest purchaser in the world of fax machines. And that just gives you some idea about the way that the NHS handles our data, right? Your competition isn't Excel spreadsheets or whatever, it's paper. It's literally piles of paper in a corner. Literally can't hack it though. It's true. So there are, you know, there are these problems, even, you know, our most trusted medical institutions just don't have access to this kind of data. But there are private companies that are working out ways to compile these vast databases. And one of them very cleverly is 23andMe and other genetic testing companies like it. So essentially what 23andMe do, as you may well know, is they charge you 150 quid and you send off your DNA. And they tell you whether you are part Viking or whatever, right? Or whether you've got the gene that makes you sneeze when you look at the sun. Thing is, their actual business model isn't the money they make on those kits. Their actual business model is that the vast majority of their customers, while signing up to have their DNA tested, also agree to have their DNA added to this database. So 23andMe over a number of years have amassed this gigantic database of millions and millions of people's data, millions and millions of people's DNA, that they can sell on to companies, private companies, research groups, for profit, essentially. That is the business model, is the data set that they end up with. They are getting people to pay to add their DNA to their data set. And there is real genuine public good that can be had from that data set. You know, having that sort of wealth of knowledge means that we can find out all kinds of things about our bodies. But I think there's also, you have to be really careful when it comes to dealing with medical data, because there is nothing more private than your DNA. There is nothing more that uniquely identifies you, the individual, and your children, and your children's children, and forever more or long into the future than your DNA. I think we need to be very careful about what we do with that data. When thinking about this exchange of data, we need to consider who benefits and how. Someone close to me has a genetic heart condition, and he has the option to get his genome sequenced. That information might be useful to researchers, and maybe one day to people with the same condition. But there are surely possible risks for him. As Hannah says, nothing is more private than our DNA, and making that information available affects our genetic relatives too. So am I worrying over nothing, or do we already have examples of people's health data being used against them? Yeah, absolutely. I mean, I think there's an example of a couple of years ago where one NHS trust was sending smokers to the back of the queue for when it came to things like knee operations. And I think that now that we're in this world where we have this incredible wealth of data, well, you know, how do we know that at some point in the future someone's not going to suggest that they'll check your supermarket shopping history before deciding whether you're allowed to have a particular type of surgery? I mean, it's already happening. There's a life insurance company in America that has announced that they're going to be linking their policies to your Fitbit. So, you know, the amount of exercise that you get changes your policy. And we're already seeing it as well with life insurance and genetic testing, that if you are shown to have some kind of a predisposition, maybe to breast cancer or to Parkinson's disease, your life insurance policy changes as a result, or you can be denied life insurance. And I think that it's this idea of things being connected in the future and something that you can't help and have no control over that's unique to you essentially being used against you. I think that's something we do need to really be concerned about. And that's sort of the next question that sort of naturally follows that is, because like you're saying, you know, in the future, this all could happen. Who decides what is allowed within these kinds of, if we talk about linking up different data sets and how hard is it? Because obviously all this technology is, you know, happening in real time and is being invented and used in real time. How hard is it to regulate this kind of stuff? It's hard. It's really hard. This is not a simple and easy problem. But I kind of think in the last few years, we have essentially been living, you know, in the wild west. Anyone could collect any data that they wanted, infer anything from it that they wanted about, about who you are, about your history, about your future, and use that to make any kind of decision and discriminate you in any kind of way. And I think that there's been no one to stop people from doing that. And worse, really, you know, that's sort of working on the assumption that these algorithms actually do what they say they do. But, you know, there's plenty of examples that I've come across of companies that just are making these bold claims that just end up being completely unfounded. And, you know, they're selling snake oil, essentially. And I like to think of the analogy with medicine really here, because, you know, it used to be the case that you could, there's one point in history where you could just essentially get any coloured liquid, chuck it in a glass bottle, sell it as medicine and make an absolute fortune. But we stopped doing that because, well, for starters, it's morally bankrupt. But also, it harms people, you know, people can end up being genuinely hurt by this stuff. And I think that really that's what's happening now with algorithms and with data, is that anyone can do whatever they want and there's no one to really stop them. And I think that what we need is we need something like an FDA, really, for algorithms. I think that we need a trusted body who can behind closed doors, while protecting the intellectual property of a company, because I think that's important to drive innovation, but can essentially assess whether the benefits of something really do outweigh the harms and whether something offers a net benefit for society. It's becoming more and more obvious that we need to think more carefully about mass data collection. After the break, we'll look at what steps societies can take to manage the use of this kind of data in future, and how we might retain control when algorithms take even more of a role in our day-to-day tasks, like driving. So if it looks like the human is about to crash, that's the point at which the algorithm comes in and applies the break or, you know, monitors the surrounding. So it's not a chauffeur, it's more like a guardian. And I think that's a much more realistic approach of both what humans are capable of and where we're at at the moment with what the algorithms can do. Don't get distracted by any bespoke Instagram ads or YouTube recommendations. We'll be right back. Last year, the Guardian tracked all the deaths of young people due to night crime and explored the themes that emerged in an award-winning series called Beyond the Blade. Why are they carrying a knife in an area where they know people, but they feel like they have to acquit themselves from other people? We saw many people suffering, but we also saw many fighting back. We've got to start looking at how we talk and how we generate and how we categorise just ordinary people that are poorer than other people or people who don't have as much as other people. For this new series, journalists from the Guardian travel to Bristol, Birmingham and Croydon in South London to listen to some of those people. Society tends to look down at young people once they've made a wrong choice. And what we're saying by that is that we're writing them up. And rather than report on their conversations, we let them speak for themselves. When I come out of jail, I'd never been praised before I turned my life around. And when I come out and got praised for the work that I was doing, I thrived. That got me to be built up a bit sooner, you know. As opposed to, yeah, just waiting to hear from here for me, because I'm waiting to hear from the next generation as well. So we're all waiting and there's no, like, action happening, happening, happening. If families are fractured, that has an impact on a young person. If a father and a mother get divorced, that has an impact on our young people. And I think the only way they know how to make people sit up and say, listen, there's a real problem going on here is by violence. To listen to all three episodes, head over to the Guardian. com forward slash podcasts, or subscribe by searching Beyond the Blade on your favorite podcast app. Welcome back to Tips With Everything. I'm Jordan Erica Webber. Before the break, we heard from Hannah Fry about the power of algorithms when combined with data and the possible consequences, both good and bad. Now we'll head back to the studio as Hannah and producer Max turn to the subject of regulation. As the general public has become more aware of examples of serious data misuse and the possible ramifications in areas like politics, there's increased concern over how vulnerable we have allowed ourselves to become and what we can expect in the future. I mean, I think it's not just the people who are involved, you know, at the top levels of the world. I think it's all of us, really. I think all of us need to be a little bit more literate about this stuff because it's making this massive, massive change. There's, you know, the shift in how we're living our lives is gigantic. You know, everything from what we're reading, what we're watching, the sort of information that we're consuming, the news that we're consuming to, you know, who we're dating and potentially in the future, even the cars that we're being driven by. You know, they're in our courtrooms, they're in our hospitals, they're in our schools, they're everywhere. And I think that we're sort of entering an era where you can't just say, oh, I don't like the word algorithm, not getting involved. Because I think this stuff is really, really important. I think we all need to be a bit more literate and a lot more sceptical. Lovely. And you mentioned cars there, and you have a whole section in the book dedicated to driverless cars. And that's preceded by, I think it's very early on, either page one or two, when you say that at its heart, this book is about humans. If we take driverless cars as a specific example, what might driverless cars teach us about ourselves, essentially? Driverless cars, for me, are the absolute perfect example of how you can't just design a technology and look at it as a separate entity from the humans who are using it. Because that, with some car manufacturers who are making driverless cars, that is essentially what I think you could argue they've been doing, where you create this technology that is very impressive, but ultimately, the algorithm is in control of the car until it's not, until an emergency, at which point an alarm sounds and then a human steps in and takes over. But I think it's worth pausing to just think about what you're asking the human to do in that situation. Because humans are terrible at paying attention to the car. We're not terrible at paying attention. We're really bad at it, right? We get bored, our attention wanders, you know, we're just really bad at that. And then what you're expecting your human to do is have an alarm go off and with zero notice for them to perform at their absolute best in the trickiest task of all with the highest stakes possible, to step in in an emergency. And I just think that's expecting too much of your humans. Now, there are other car manufacturers who I think are taking a much more responsible approach to this. Because until cars are amazing, right, once cars are absolutely perfect, we don't need to worry about this, it's the transition between the two. There are these other car manufacturers who are, I think, taking a much more sensible approach to this, which is switching the role of driver and car around. So switching the role of driver and algorithm around. So rather than having the algorithm driving the computer with the human there to step in, it's playing more to the strengths of the human and the algorithm. So the human is still the driver. And then the algorithm, which can work tirelessly in the background, continually checking everything, continually checking the surroundings, the algorithm is the one that can step in in the case of an emergency. So if it looks like the human is about to crash, that's the point at which the algorithm comes in and applies the brake or monitors the surrounding. So it's not a chauffeur, it's more like a guardian. And I think that's a much more realistic approach of both what humans are capable of and where we're at at the moment with what the algorithms can do. So while we increasingly look to technology to compensate for the deficiencies of humans, we must also be aware of the limitations of that tech in situations that might be more appropriately managed with a human touch. I'm a philosophy graduate, and one of the philosophical thought experiments we all learn about is one in which a runaway train will kill three people unless you divert it to a different track where it would kill just one. You might have seen the basic idea explored in popular video games as I discuss in my book, 10 Things Video Games Can Teach Us, or on my current favourite television show, The Good Place. Essentially, you're asked to consider, in a situation in which you could save several lives by sacrificing just one, would you? And would that be the moral choice? This dilemma is referred to as... Classic sort of, um, trolley problem. Yeah, how's it going to decide between, you know, killing one person versus five people? It kind of highlights that aspect of... There are some, you know, aspects of human cognition, say, that we haven't got to a point that we can reduce or produce an algorithm to do that. Do you think we'll ever get to a point where it will be able to do that? The whole point about the trolley problem when it was first, um, devised is that there are some questions that don't have a right answer, right? There are some questions, moral questions, that are too tricky to say, that's exactly what you should do. And I think that, you know, if you've got a human driving a car who has to make that decision, that snap decision about what to do, you haul them up into a courtroom afterwards and you say, why did you do that? And ultimately, they can defend themselves because they can say, I just, you know, it was an instinctive thing and I just made a decision and I, you know, I can't explain my cognitive process really, I can't unpick it. If you've got an algorithm in that situation, then you can go through line by line through the algorithm and work out exactly at what point it made that decision to kill a particular individual. And I think that there's a, there's a really big story there about how much we are handing over control to systems that are ultimately flawed and how we're trying to squish these logical systems into the real world, which just doesn't really work that way. I think that really illustrates what the problem is with trying to force that algorithm shaped pig into the human shaped hole. It just doesn't, it doesn't really work. There are these gaps at the edges and I think the trolley problem is a really clear example of that. And that's sort of something that I was hoping to ask you, you set me up very nicely, is this idea of reducing the very complex and messy and, you know, flawed world of humanity into this supposedly kind of neatly rational algorithmic world. I mean, firstly, it makes a lot of people uncomfortable, but I suppose you've kind of answered my question, but you're a mathematician by trade. That's essentially what you try and do in maths, right? You try to reduce the real world, but I'm guessing you don't think that all of it can be reduced to that kind of digital space. I absolutely don't think that it can all be reduced to that digital space. I mean, I don't get me wrong. I think that there's an incredible amount that you can do. And I think that there's, you know, you can make real benefits to society at large if you think more rationally, if you work things through more logically, and, you know, you can be more efficient, you can manage your resources more effectively. I think there is genuine, huge, huge positive benefits to be had, but you can't be naive to the limitations of what you're trying to do. And I think that that is where I'm worried because I think that there are people who are naive to the limitations of what's already been created and the dangers of what we have already created going forward. And that I think was really the reason why I wanted to write this book, just to explore where we are right now and whether we're sure we're comfortable with where we're going. And it's because it's kind of a naivety on the side of the program is about what the limits are. But I also think what you end up doing is, because people are very uncomfortable with this idea of reducing everything to the algorithmic world. And so you also have a naivety on the other side, the side of the user, which I think ends up making people feel very threatened because they feel like that is what programmers are trying to do. They're trying to reduce every single aspect of our lives. And rightfully so, as you've just pointed out, people don't agree with that or feel it's right because we want to maintain some of that messiness. We want to maintain some of that, you know, flawed nature. That's what being human really is. Yeah, we need to make sure that we still feel like we're individuals, that we still have, you know, free will. I don't want to be reduced to a number any more than you do. You can't take humans and sort of put them into little boxes and expect them to live there. Just doesn't work like that. But you know, at the same time, I also know that when I'm getting the tube in the morning, I could make a very good prediction if I had access to the data of exactly how many people are going to pass through that station and the different routes that each of them are going to go on. And I could design the building to accommodate that prediction of how this big group of people are going to behave. And that's really, I think, where the maths-y side of it fits in really well when you're thinking about these big problems and how to make things as efficient as possible. And not sort of about, well, trying to distill human character into a number. So if you do it, it's fine. If anyone else does it. I mean, that isn't what I said, but it's definitely what I implied. As with any term that enters the public consciousness suddenly and at scale, some people are sick of hearing about algorithms. But as Hannah has illustrated in her book, it's important to keep an eye on these systems and the power they, combined with big data, have over our lives. We shouldn't overestimate their capacity to adequately capture the very nature of a human being or to completely run our lives without a human input, but we mustn't let our apathy lead us to underestimate them either. I'd like to thank Hannah Frye for joining us on the show this week. We'll include a link to Hannah's book in the episode description on the Guardian website. If you want to talk more about the trolley problem or let us know what tech development has got you thinking philosophically, please email me on chipspodcast at theguardian.com. This episode of Chips With Everything was produced by Eva Krizjak. I'm Jordan Erika Webber. Thanks for listening.