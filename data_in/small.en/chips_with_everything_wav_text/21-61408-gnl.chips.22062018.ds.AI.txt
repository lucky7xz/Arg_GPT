 The Guardian. In 2013, President Obama defended the use of drones in the war against terror. From the very beginning of his administration was sort of a drone president. You know, he radically intensified the strikes in Pakistan, expanded the drone strikes to Yemen and Somalia. He was forced to justify drone strikes that had killed four US citizens. Critics hoped that this news would deter future military use of this kind of technology, but instead the government has opted to develop it even further, using artificial intelligence. Enter Google. In April 2017, the US Department of Defense launched an algorithmic warfare cross-functional team, otherwise known as Project Maven. The project uses Google's artificial intelligence to analyze drone footage. This did not go down well with Google's employees. More than 3,000 workers signed an open letter to Google CEO Sundar Pichai, asking the company to pull out of the program, writing, Google should not be in the business of war. In early June 2018, reports surfaced that Google had informed employees it would not renew its contract for Project Maven, and mere days after Sundar Pichai published a blog post that might explain why. The post, titled AI at Google, Our Principles, laid out seven objectives for AI applications to guide Google's work going forward. According to the list, Google believes that AI should... Be socially beneficial. Avoid creating or reinforcing unfair bias. Be built and tested for safety. Be accountable to people. Incorporate privacy design principles. Uphold high standards of scientific excellence. Be made available for uses that accord with these principles. On top of that, Pichai listed four AI applications that Google will not pursue. One of which was weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people. But Google's current contract with Project Maven won't expire until 2019. So when does its CEO envisage adopting these principles? How will these objectives actually work in practice? How will we know that Google is following them? I mean, I know these are very, very, very general, but they have principle four, be accountable to people. Now, I think the suggestion there, that's very much linked to this issue, which is that these ethical decisions will need to have input from the humans on an ongoing basis. And is this the beginning of a more responsible AI industry? I'm not saying you should regulate everything, but to a certain extent, if we have real problems, then yes, we probably need some regulatory oversight for those things. I'm Jordan Erica Webber, and this is Tips With Everything. Sorry, I interrupted you. That's OK. I didn't realise either. That's OK. Things get picked up on all of these microphones. This week, we're doing something a bit different. We've brought together some of the best minds across tech and ethics to discuss ethical standards for the research and development of AI. So this week, I'm joined in studio by Sanjay Modgill, a senior lecturer from the Reasoning and Planning Group at King's College London, Dr Yasmin Erdan, a senior lecturer of philosophy at St Mary's University in London, and on Skype, we're joined by Dr Sandra Wachter, research fellow in data ethics, AI, robotics and internet regulation, cybersecurity at the Oxford Internet Institute and the Alan Turing Institute in London. So that's three people who are incredibly qualified to help us discuss this week's topic. I should also point out that we did reach out to DeepMind to see if someone from the company wanted to discuss this with us, but they weren't available at this time. So Google's CEO Sundar Pichai, or at least someone from his office, published a blog post in early June that outlined Google's seven principles for working with AI, plus four areas in which they will not use AI. You've all had a look at them, and we'll get into these specific commandments that intrigue you in particular later on. But first, Sandra, just from you, could we get some context for the conversation? So just could you remind us of Google's history with AI? Yes, so I think Google in particular, but actually all major tech companies have become very aware of the ethical implications of AI. And for quite some time, I think everybody was very keen on finding a way forward to at one hand harness the full potential of AI, but at the same time find solutions to mitigate those risks. And I think the statement now is a very good step forward to actually pledge that some ethical consideration will be at the heart of their work. I'm interested in why Google is doing this now. Have they got into trouble before for the AI research? Anyone? Well, it does seem as if the staff rather rejected their work on military drones, and I think that that puts enormous pressure on them to do something. Is that Project Maven? That's right. I think also there has been some talk recently about the fact that the don't do evil principle has been quietly dropped from their commitments. Don't be evil had long been Google's unofficial company motto, but when the company was reorganized under its parent company Alphabet, it was slightly adjusted to do the right thing. So Yasmin, it's not the first time that rules or principles have been written up by organizations that are concerned with using AI responsibly. In fact, the Future of Life Institute, for example, has had hundreds of AI researchers sign its list of AI principles, including many Google employees. So why did Sundar Pichai decide to write his own list, and why right now? I mean, it seems like a very good step forward. I can't help but be slightly cynical, and it does seem to me rather more of a PR sort of offensive than much else. For instance, I'm quite concerned they're called objectives, which doesn't suggest to me an intention to make this, you know, something concrete that you must abide by in the same way that something like rules or laws or principles, as you described, would suggest. Yeah, I think they do come across as rather vague, and of course, the devil is always in the detail. But I do think it's a step in the right direction. Let's hope that there are some follow-up work on these principles. They're made more concrete, that there does prompt a discussion as to what the appropriate regulations are. And I think we really have to see now how these principles are now fleshed out in a bit more detail. The very vagueness of the principles means that you might follow them, and inadvertently, that may still lead to harmful consequences. I mean, in some sense, this is the rhetorical thrust, ironically, of Asimov's three laws. If we encode these kinds of principles in the AIs themselves, they may still eventually cause harm, even though they abide by those principles in their general sense. Before we started recording, I asked each of my guests to pick one of these seven objectives that stood out to them as particularly interesting. So we're going to dig into some of these principles in more detail, and we'll start with you, Sanjay, of these seven commandments, or objectives, as they're called, laid out by Pichai. Which one in particular intrigued you? I started with the one on being built and tested for safety. So, of course, there's a major issue and a major concern about the safety of artificial intelligence. What interested me was many of these doomsday scenarios have anticipated problems arising when these artificial intelligence technologies are developed by the companies. So what strikes me as being a difficult issue would be when the artificial intelligent machines start to be used by Google DeepMind. They start to become more autonomous. They start to engage in research and development themselves. And then we might have the problem that the actions of these machines will not be aligned with human values. So that goal is a very complex goal to unpack and to make sure that it is actually satisfied. So that's principle number three, the built and tested for safety. And I'll just elaborate on what Pichai writes there. He says, we will continue to develop and apply strong safety and security practices to avoid unintended results that create risks of harm. We will design our AI systems to be appropriately cautious and seek to develop them in accordance with best practices in AI safety research. In appropriate cases, we will test AI technologies in constrained environments and monitor their operation after deployment. Yasmin, it seemed like you had something you wanted to say there. Well, I think one of the problems there is, again, in this terminology that they're adopting, they talk about being appropriately cautious. And I mean, that leaves so much room for ambiguity and what we think about as appropriate and in what context and according to what needs and interests. Also, to be cautious doesn't mean that they don't do something. You can still do something and be cautious about it at the same time. So it doesn't really commit them to all that much. And I have the same sort of concerns with a lot of these objectives. But when it comes to safety and security, that's really a very big problem. Yeah. Does anyone have any thoughts for how you actually find out if an AI is safe? Maybe it's not so much a question of finding out how an AI is safe, but ensuring that an AI is safe in the sense that in order to make an appropriate ethical decision, one would require input from the humans. I quite like the idea that they had, I mean, I know these are very, very, very general, but they have principle four, be accountable to people. Now, I think the suggestion there, that's very much linked to this issue, which is that these ethical decisions will need to have input from the humans on an ongoing basis because of course our moral and ethical systems evolve over time. It's not like you can prescribe something from the outset and then let it go and assume it will be appropriate. Does anyone have any thoughts on whether this will affect things like autonomous vehicles? So the idea that something has to be tested for safety, if it has the potential to kill people, does that mean you just shouldn't work on it at all? I think there is, what's very important to distinguish is research versus deployment, right? I think those things are very often mixed together and that's not actually accurate. You can research and test a lot of things before you make the decision to deploy it. So when it comes to critical safety systems like autonomous cars, I didn't think anybody would argue that we shouldn't be testing them. In fact, everybody would agree that we should test it in a lot of different and diverse environments to make them learn from the mistakes and stuff like that. The other question is, when is a product safe enough to be released to the market? And I think this is where we probably, this is where the regulation stuff starts, not so much with the research actually. Do you think that an outside body should regulate the safety of AI at Google? I don't think there is like a one-fits-all solution to that. I guess there are algorithms that have not so many risks attached to them, where we might not need oversight, but for certain areas, I think it might be irresponsible to not have oversight. I mean, we have cars, right? We have public bodies regulating how to use cars and we need to get certificates for being allowed to use them on the street because there are risks attached to it. So if there is a deployment or a system that has similar issues, then yes, of course we need oversight from a governmental or independent body. Yasmin, you mentioned Project Maven. If Google was following this principle of making sure things are tested for safety, do you think they never would have been able to pick up that contract in the first place? Well, I think in a way they've sort of left space to do that because they've said that, you know, they're not going to be involved with military equipment per se, but they did say they would still be working with governments and they gave a list of ways in which that might play out. So again, I'm not sure that any of these objectives or present a particular barrier to that sort of work. Of course, an interesting thing would be, well, what if Google were involved in, let's say, the development of a certain kind of sensor technology that was extremely accurate at identifying an ISIS terrorist? Now, that sensor technology could be used by an autonomous weapon, which was not developed by Google, but it still makes crucial use of a technology that's been developed by Google. Now, maybe this principle allows them, gives them the leeway to develop such a sensor technology, and one furthermore could argue that on a utilitarian grounds, that actually on balance that would be a good thing. What actually would it be detecting when you say a terrorist? I mean, it's a thought experiment, if you like. I mean, I'm a philosopher, so I love thought experiments, but I would also say that there is a real problem with them. I think that they sometimes seduce us into thinking that a scenario that could, in principle, at least in our minds, be real, therefore has some possibility in reality. And the risk with that is if things happen as a result of that possibility. So I think we need to be really careful about either suggesting what AI could do in a way that actually is not going to happen or suggesting what it could do, and therefore we need to take account of that in terms of what we think about ethically and in legal terms. So I'm not really sure how technology resolves the problem, which makes good use, or bad use, if you prefer, of the sorts of problems that we have in our human engagement and then feeds them into technology. So it's difficult to envisage exactly how Google will make sure the AIs they deploy are fully safe, but a plan is in place. And as Sandra explains, the blog post is symbolic. It represents the beginning of an important conversation. After the break, we'll discuss Google's ongoing issue with unfair biases in their AI software and what it means to be accountable to people when working with AIs. Up until this point, from a legal perspective, there wasn't much that you could actually do because we really don't have any laws governing AI. We'll be right back. We can travel far from home just to become a doctor. And how famines and a lack of basic medical care drove him to create a foundation that would eventually train villages to become community health workers. To have a listen, head over to theguardian.com forward slash podcast or search small changes on your favourite podcast app. Welcome back to Tips With Everything. I'm Jordan Erica Webber. Before the break, I and a small group of experts started to dissect a recent blog post from Google CEO Sundar Pichai, which outlined the technology company's seven objectives for responsibly working with AIs. It seems pretty obvious that AI can reinforce bias in some pretty horrific ways. Just this week, my friend Shella, who actually co-hosted this podcast for our Black History Month episode, tweeted that Samsung had created a storyline from her photos called My Furry Friend that included photos she'd taken of animals and one picture of her with her hair out. Google has also made some serious mistakes on this kind of thing, but do you have any ideas how Samsung, Google and everyone else can actually avoid reinforcing unfair bias? What do you think, Sanjay? Do you think it's possible to avoid bias with this kind of tech? I think the paradigm in artificial intelligence is machine learning, which is essentially learning from precedent, from the data. I think this is behind principle four, which is corrections of these biases will require that the machines explain their reasoning and make their reasoning transparent. My understanding and hope is that they are aware that it's a problem and they are working on solutions, albeit they may not have developed particularly sophisticated solutions and in particular the problem of course is with explaining the reasoning. But that's my understanding of how they're looking to address this issue. What strikes me is that it is phrased as we should minimize biases. I think it would be smart to say we should increase diversity and I think that would be actually helping the problem because the problems of bias and discrimination is partly because the data is biased and it just replicates the stereotypes that we have. We also have a problem in the coding community in terms of biases because the coding community is not very diverse. So obviously those people have intentional and unintentional and latent biases. They are just going to, you know, everybody has certain convictions and this will be fed into the algorithm as well. And the last thing is diversity in terms of disciplines. There is probably not just a tech solution to the problems that we're facing. We actually need different disciplines to think about this together. When you talk about the political impacts of AI, political scientists are important. Are you talking about the economic impacts of AI? You probably need economists for that. Is it about the legal implications with lawyers? Is it about the ethical implications? You need philosophers. So it's not just about tweaking the algorithm to make it seem unbiased. It's actually a holistic project that you need to embrace and that's diversity. I mean, perhaps again this will be a place where regulation could dictate that there is the kind of, for example, the required diversity in the bias correctors. You've got this team of bias correctors that say, no, no, no, this is bias. And regulation could help there to say, okay, well, you need to make sure you've got a full, you know, representation of different cultures and genders, etc., etc. And that's where regulation could really make a difference. So our panel agrees that when it comes to AI's creating or reinforcing unfair bias, we can't solve the problem with more technology. We need to look to human beings. But what happens when issues do arise? Who is held accountable and how? Which brings us to Sandra's chosen objective. So the issue of accountability has come up quite a lot already in this discussion. But Sandra, you wanted to dig a little deeper into this, which is principle number four, which is be accountable to people. So Pachai writes, we will design AI systems that provide appropriate opportunities for feedback, relevant explanations and appeal. Our AI technologies will be subject to appropriate human direction and control. So up until this point, how has this kind of thing worked? If people had complaints about AI, how did Google deal with them? So far, there's been a very vivid discussion about accountability in general, because you know, the algorithms are very unpredictable in what they do. So you don't really foresee how they're going to make decisions and what they're going to do. If they do it, you might not be able to understand it. And they work very anonymously. So it's basically the algorithm doing stuff and not necessarily human and all those things are very often used to say, well, you know, it's not the human's fault, the algorithm did it. And that's a very bad, bad excuse. And I think an excuse that we should not accept in a society, the whole computer said no, and I don't know how it works. But let's just move on. Let's not try to open up the black box is a very big problem. Up until this point, from a legal perspective, there wasn't much that you could actually do, because we really don't have any laws governing AI at all. We do now have the new data protection framework in Europe. But it's a very good step forward in ensuring explainable and transparent AI and giving users or the general public a way to start engaging in dialogue rather than saying this is a black box, we don't understand it. And that's the end of the discussion. And in fact, if you were to ask someone who was a researcher in machine learning, well, can you explain why your machine learning algorithm made the decision it did, they'd be flummoxed, frankly. So it's one of the most pressing problems. Yeah, it's still quite woolly. It offers a very promising intention. But yes, I'm I certainly can't see what that concrete outcome could look like, especially when you take into account quite how many different problems may arise and do arise in response to these sorts of AI systems. Sandra, do you think that this goal would be better achieved if the tech industry as a whole just kind of came together and called for a regulatory body to specifically look at companies that create AI? Yes, I think, even though I think like the first step forward was very good of Google to say what they think how the industry should move forward, I think the next step is actually to start a conversation with the whole of industry. For example, the partnership on AI would be a very good place to have that. And I'm very much hope that this will actually start a fruitful conversation, not just with the tech industry, but also with civil society and academics and government around that. I'm not saying you should regulate everything. But to a certain extent, if we have real problems, then yes, we probably need some regulatory oversight for those things. So this list of objectives doesn't seem to provide as many concrete answers as we might have hoped, though it's certainly a start. But at least until the end of its current contract with Project Maven, Google's AI will still be used in a way that could be seen as violating its own principles. I'd like to thank all of my panel guests for joining in on the discussion this week. There will be a link to the original blog post from Google's website in this week's episode description on the Guardian website. Before we sign off, our interesting tech fact of the week is about a subject close to my heart, video games. The World Health Organization is in the process of updating its diagnostic manual, the ICD, and the new edition includes a new mental health condition called... gaming disorder. The disorder is characterized by impaired control over gaming, including prioritizing playing games over other interests and activities, and continuing to do so even if there are negative consequences. To get a diagnosis, there has to have been at least 12 months of significant impairment to other areas of a person's life, like family, school, or work. To let us know your thoughts, or to share any interesting tech stories you think we should cover, email us at chipspodcast at theguardian.com. I'm Jordan Erica Weber. Thanks for listening.