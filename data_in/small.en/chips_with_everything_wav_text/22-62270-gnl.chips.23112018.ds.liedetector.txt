 The Guardian So Jordan, I have a game for you. It's called Two Truths and a Lie. And you have got to guess which one is a lie and, well, which ones are the truth by default. You know, guessing games are like my least favourite kind of game. Well, here you go. Here's the first one. I once walked for a mile with my skirt tucked into my knickers. Oh, that I can believe that's happened to me. I'm named after an alcoholic drink and the third is I've broken both wrists and both ankles. At the same time? No comment. No comment. Do I have to guess now? No, I'll tell you what, I'll tell you at the end. OK, I'll mull it over. How good are you at lying? Could you fool a friend? How about a machine? We've recently learned that the EU is about to start trialing an artificially intelligent machine or as they call it, deception detection, which is supposed to be able to detect when someone is lying at border control. The outcome is absolutely crucial. We're playing with people's lives here, so that's why we have to be 100 percent sure about the technology. I'm not convinced at present that anything is truly 100 percent. We don't ever going to expect complete accuracy because it's never actually going to happen. But what we want to do is follow an ethically aligned design approach, take into consideration that we need balanced data sets for training and testing and we will continue to show transparency of our results. And it got us wondering about lying, about how we learn to lie. If an intelligent machine can pick up on clues and cues, can we trust its judgement? I'm Jordan Erica Webber and welcome to this special edition of Chips with Everything. This week we've teamed up with Grae Jackson from Science Weekly. Yes, you have indeed and we're calling this collaboration Science with Everything. My name is Dr Paul Seager. I'm a senior lecturer in social psychology at the University of Central Lancashire. I'm interested in lie detection, how good we are at detecting the deception of others and why we fall for people's lies and I'm also interested in con artists and how they make us fall for their lies, if you like. I thought a good place to start with Paul was how we learn to lie as kids. I wanted to know whether it's something ingrained in us or something we glean from our parents. We have something called social learning theory which suggests that we learn our behaviour from other people from significant others and significant others pretty much are parents to young children. So if they see their parents lie, they are more likely to do it themselves. So you can imagine the situation, we're coming up for Christmas, a young child gets a Christmas present they don't particularly like, they turn around to Auntie Louise and say, I don't like this Auntie Louise and everyone shrugs and raises their eyebrows and the parents take them to one side and say, well, you didn't have to be quite so honest. And it's that kind of behaviour and talking to from parents that children start to think, well, okay, so I don't have to be honest to Auntie Louise all the time and they kind of take that one step further and think, well, if I don't have to be honest to Auntie Louise all the time, maybe I don't have to be honest to everybody else as well. I read this wonderful study about a child is in a room and there's a toy underneath a blanket and the researcher goes out the room and says, don't look under the blanket, we'll do that later. Sure enough, the kid obviously looks under the blanket and it's a Barney the dinosaur. And the research comes back into the room and says, okay, you're allowed to feel it, what do you think it is? And the child says, oh, it's Barney. And the research says, oh, well, how do you know that? And she says, because it feels purple. And she's sort of aged sort of, I think four or five in this study. So we obviously start off bad at lying, but we become much better at lying. And I wonder, is that just through learning to lie better or is there something going on? Oh, Dev, oh gosh, I can't say. Developmentally. Thank you, developmentally. There are, yes. There's two things going on there. The first one is perspective taking and the second one is good communication. Now, perspective taking is where you are able to put yourself in the other person's shoes. So in the example that you've just outlined, if the child had been able to put themselves in the other person's shoes, they would have known what the other person knew and what they could know and all those kinds of things. And good perspective taking doesn't really mature until generally about the age of six. If you link that in with good communication, children tend to start developing fairly good communication around about the age of three. And as they get older, then they get a bit more sophisticated. So if you take the two things in tandem, if you like, the perspective taking and the communication, by about the age of five to seven, they are starting to able to hold their own when it comes to telling lies. Now, some of the skills will be more developed than others and it will be a very idiosyncratic thing about exactly how these skills develop. So for example, you might have a child who's very good at perspective taking. So they could say, if they're accused of taking a cake, for example, they could say, yes, mummy, I understand why you might think that I took the cake because there's nobody else in the house. But actually, I can assure you that you're mistaken. And then because their communication has fallen a bit behind, they might follow it up by saying, I promise I'll never do it again. So the two skills that perspective taking and the communication have to be in tandem and they have to draw together, if you like, for a successful lie. So, Greya, as we develop, our lies become more sophisticated. And that's because children start to communicate better and become better able to put themselves in somebody else's shoes. Yeah, it's something called theory of mind. When we begin to understand the beliefs, knowledge and intentions of others, our fibs become more convincing. All of which Paul says doesn't get good until six years old. That said, other experiments have shown that certain aspects of theory of mind could develop earlier than that. A couple of studies even suggest babies as young as six months old can fake cry. So maybe some forms of deception start from the word go. Right, so is lying an innate skill, something that you're born with, or do babies maybe learn that they can get an adult to pick them up if they cry whether the crying is real or fake? Yeah, good point. Either way, what we do know is that as we get older, we get better and the lies become harder to detect. I think it is actually quite difficult there's been a number of studies that look generally speaking are accuracy levels. And there's a there's a brilliant study that suggests we're 54% accurate. Now what does 54% accurate mean? If you're faced with somebody that you don't know whether they're lying or telling the truth, if you flip to coin, heads they're lying, tails are telling the truth, you'd be 50% accurate. So the study suggests that we're actually no better really than chance in detecting other people's deception. What we tend to find in the research is the closer we are to somebody relationship wise, the worse we actually are detecting them. And that also tends to be the case when we don't know the person. So when we're absolute strangers, because we don't know their idiosyncrasies, we don't know how they act when they're being honest. Now somewhere in the middle, you have the sweet spot where you know somebody reasonably well but you're not too close to them. Say a friend, for example, we tend to find that we're better at detecting deception, lies and truths from our friends than we are from our spouses or from strangers. So clearly there are some cues. What are those cues? Are they the sort of the same for everyone or are they very different? It seems to me you're suggesting they're quite different. We tend to break the cues down into non-verbal communication and verbal communication. Now non-verbal communication tends to be your basic body language. So things like eye movements, arm and hand movements. And then we have the verbal cues. So what you say and the way in which you say it. The body language, the non-verbal communication tends to be, in my opinion, very unreliable. And I always train people to pay attention to the verbal and vocal cues. For example, there tends to be a good body of research that suggests that when we're lying, our voice pitch goes up ever so slightly. Now that's not something that we would immediately be able to recognise unless we're actually listening for it and unless we know how people talk when they're being honest. But that's just one of the vocal cues. It also gets slightly more complicated. We have to take into account about how long somebody has had to prepare their lie. So if I kind of put you on the spot and asked you a very difficult question that you might not want to tell me the truth about, what I would probably tend to find in your deceptive answer is that it would be fairly short and it would be fairly general. Whereas if you had had a long time to prepare the lie to the question that you knew was coming, so for example, in police custody, suspects that are brought in know the type of questions that they're going to be asked and they can prepare for them, those type of cues would be a little bit more difficult because they would tend to give longer answers and more detailed answers because they're prepared for them. But then there are things that we can do to get around that and so it goes on. What we think of as a very easy topic tends to be actually a very complicated topic and so there are no straightforward quick answers. The more I hear Paul talk about detecting lies, the more I realise just how complicated lying is and how much it depends on context. Yeah, I know. From what Paul is saying, it depends on how well you know this person and how well you know their tics. But then also if you're too close to that person, then they may be able to manipulate you more and make you believe the lie. So there's that and then there's also how long they've had to come up with the lie. If they've had a long time to think about it, then it's harder to detect. But having said all of this, I think it's really important to highlight that according to some studies, lying can be like a social glue to help us cooperate as a species. Think about white lies. We tell them all the time. You flip to your boss and tell them you like their shoes, or you bail on a date because you had to work late. And really you just stayed in and ate takeaway. Yeah, people who know me personally will know that one of my defining traits is honesty, and that means I sometimes tell the truth when I probably really shouldn't. But there are situations in which deception is dangerous and that's obviously where this artificial intelligence in border control comes in. But everything you've told me so far about how hard lies are to detect does make me wonder how we could expect to train an algorithm to do it. This system is not designed to replace border guards and we believe that computation intelligence systems such as this operate best when they inform people who are educated about how to actually use them. After the break, Jordan and I will be looking at how researchers are using artificial intelligence and facial recognition technology to figure out if we really can be trusted to travel. We'll be back in a minute. Today in Focus is a new Guardian podcast that brings you closer to our journalism by getting behind the news every weekday. You'll join me, Anushka Astana, talking to people at the centre of the big stories impacting our world. We'll use personal perspectives and expert analysis to put you at the heart of what matters. Listen to Today in Focus and subscribe on Apple Podcasts, Spotify or wherever you choose to listen. Welcome back to Science with Everything, I'm Grey Jackson and I'm Jordan Erika Webber. Jordan before the break, Paul talked about how our parents inadvertently teach us that it's okay to lie as kids and as we grow up our deceptive abilities become more refined. But our ability to detect lies doesn't really improve with age. Paul said our chances aren't much better than flipping a coin. So if it's so hard for humans to tell whether or not someone is telling the truth, I wonder whether a machine could do it better. At the start of the show we mentioned that soon the EU will be trialing an artificially intelligent machine. It's designed to see if those passing through border control are lying or not. And it's called iBorderControl. Hello it's Keely Crockett, can you hear me? Hi Keely, we can hear you loud and clear. How are you? Oh I'm not so bad, thank you. It's good to speak to you. Dr Keely Crockett is a reader in computational intelligence at Manchester Metropolitan University. She's also a researcher on the iBorderControl project. Where about to be talking to you from Keely? You're talking to me in the School of Computing and Maths at Manchester Metropolitan University. Starting at the end of this month, the pilot will run for six months at four crossings in Greece, Hungary and Latvia, which all have borders with countries outside of the EU. The pilot technology includes virtual border guards that can interact with the traveller in Hungarian, English and Russian. I'm struggling to imagine what it might be like if I was going through a border control like this. I know you have some sort of application to do beforehand, but what's it like when you go through? Do you know? I will let Keely explain. Step one. Travellers in the comfort of their own homes will register on an online system that will collect relevant information about themselves and their trip. And then each trip that that passenger goes on is actually registered. If we consider as it being self deployed through the pre-registration phase, it may, and I'm going to stress the word may here, be able to evaluate travellers who cannot be evaluated deterministically by other methods. So therefore it may prove to be a key enabler in identifying subjects that border guards should pay special attention to during the actual crossing. Step two. So during the pre-registration step, an avatar border guard will ask the traveller a series of questions about their trip and what it does, it records a deception risk score for each question and then for the overall interview. The avatar currently has been implemented in both male and female genders in English, Hungarian and Russian. Step three. Once they've had the interview and everything is completed at pre-registration, they will receive a QR code to their device for the trip they're going to take. Step four. And at the border crossing stage, iBorderControl will provide the technology to the border guards, which is going to be integrated either into their existing installation or in a new portable hardware platform that they're going to wear, so a wearable system. And all information travelled during pre-registration will be there for the border guard to see to support him making the ultimate decision. All the biometric texts will also take place if required, such as the palm vein face matching, which is a new technology, fingerprints, you name it, all the things we would come to expect at border crossing point. Can I just check on something you said earlier in that answer? So you mentioned that there's a possibility that this technology would be able to detect deception in ways that other methods wouldn't or better than other methods. So what do you mean by that? Do you mean that machines might be better at this than human beings? So the question we have to ask ourselves is, a border guard may be absolutely fine, have no biases, etc. He has his subjective opinion of someone coming through who answers those questions and he makes a decision or she makes a decision at that moment in time about whether they need to go to a second line check for further questioning. What we have here is a system that operates on non-verbal behaviour in individual, so it has no subjectiveness in it. This system is not designed to replace border guards and we believe that computation intelligence systems such as this operate best when they inform people who are educated about how to actually use them. We believe that people tend to have some kind of bias, this may be an unconscious bias and they're not really aware of it, whereas with our system we are not introducing any subjectiveness at all. Okay, so let's talk about the technology itself then. So it uses artificial intelligence, right? So how exactly does it incorporate AI? The technology itself was based upon a system known as silent talker which we've developed several years ago and what it does, it combines information from typically 38 to 40 fine-grained non-verbal channels from the face simultaneously, uses artificial neural networks and what it does, it tries to generalise about deceptive non-verbal behaviour. It assumes that certain mental states associated with deceptive behaviour will drive an interviewer's non-verbal behaviour when they're deceiving and this can include stress or anxiety, arousal, cognitive load, behavioural control, duping delight etc. And it takes candidate features as the input and determines itself what the interactions are between them over time which typically could indicate patterns of lying. Can I just jump in here because Paul trains people in detecting deception and he said before that non-verbal cues weren't a very reliable way to detect a lie. And the other thing I think we should mention here is that there was an article published in The Guardian and it quoted several scientists criticising the technology saying that for example, micro-expressions don't actually tell us if someone is lying or not. One scientist even went as far to say that this is everything that can go wrong with lie detection. Did Keeley have any sort of rebuttal to that? Oh, she sure did. People who say that typically have been referring to the use of micro-expressions and micro-gestures, what we use, are significantly different because they're much far fine-grained and they don't require no functional psychological model of why the behaviour has taken place. So because we actually believe it's a combination of these micro-gestures that work together to determine the pattern of behaviour whether truthful or deceptive. And we capture that over a time interval, say one second or three seconds, we can link these micro-gestures together. So what we can say so far in terms of this difference is that we've collected evidence that supports the hypotheses that non-verbal behaviour can be used to detect deception at levels statistically greater than chance. The use of facial recognition technology also raises some questions. In October of this year, the American Civil Liberties Union called for a temporary ban on the use of this kind of tech in immigration enforcement and law enforcement and asked the Department of Homeland Security to disclose how they intend to use it. As the ACLU Senior Legislative Council, Nima Singuliani, pointed out, history tells us that surveillance technology is often wrongly used to target immigrants, communities of colour and political protesters, and there's a danger that this time will be no different. But the team behind iBorderControl are apparently fully aware of the issue of bias when it comes to this type of technology. We do believe, and we agree with the people who work in the field generally, that we need to have balanced data sets with equal representation of all the different subgroups that actually occur and we need to make sure our results are transparent. Not only that as well, it's all about really education. We have the border guard who is a human in the loop person and what we want to do is to make sure they understand the positives and the limitations of the actual technology. So it's not a case of saying, here it is, off you go. It's a case of making sure that they understand, bearing in mind that our automated deception detection system is one part of a larger wheel that makes the ultimate decision. But it's so they understand during training what it actually means. I think the key question here is whether we can really trust technology like this. This is something I put to Paul Seager. I wouldn't like to say one way or another whether AI will be able to get sophisticated enough to be able to detect whether somebody is lying or telling the truth to them. It's beyond my speciality. I'm not sure about computers and algorithms and all that kind of stuff. All I would say is if it's important that the AI detects us, we need to be very, very sure that the AI is going to be accurate because if they revert to anything like the polygraphs and the voice test analyzers, we will find lots more false positives than we should actually be finding. And I suppose in this particular example, actually, that really matters. It builds a control, that decision of whether you're lying or not. Actually, I'm thinking with asylum seekers, all sorts of things. Those things really matter. Those decisions matter. That's right. The outcome is absolutely crucial. We're playing with people's lives here. So that's why we have to be 100% sure about the technology. I'm not convinced at present that anything is truly 100%. Would we be prepared to go for 95%? Well, maybe, but that still means that 5% of people are going to be disadvantaged to some degree. So I asked Keely, will we ever get 100% accuracy? We don't ever going to expect complete accuracy because it's never actually going to happen. But what we want to do is follow an ethically aligned design approach, take into consideration that we need balanced data sets for training and testing, and we will continue to show transparency of our results. Can you put a kind of percentage accuracy on that? So you've said it will never get to 100% accuracy, but is it like 50, 60, 70? I can say that when we first started this in 2016, we published some early results the following year that indicated with our small sample, which were mixed gender and also mixed authenticity, we were getting around about 76%. Previous work on other studies that we've done with the silent talker component has got up to 87%. So we would look for this project to try and aim at least for 85%. And since those results were published, we're quite near there. So Jordan, given that both of our guests have explained why it's difficult for both humans and machines to determine whether you're lying or not, were you able to tell which one was a lie and which two were the truths? Or would you rather enlist a machine to help you? I know I have fallen foul of a fair few liars in my life, so I clearly could do with some help. And I think, you know, as long as the machine doesn't actually have any real decision making power, I'd happily give it a go. So just to recap, the three statements I gave you were one, I walked down the road with my skirt tucked into my knickers. Two, I'm named after an alcoholic drink or three, I've broken both my wrists and my ankles, possibly at the same time, possibly not. So one of them is a lie? One of them is a lie. Honestly, I think given that Grey is a name that not very many people come across, I feel like that might be something that you say because you're bored of people asking about your name, but it's actually not true. Is that a lie? So no, my middle name is Laurel, like Lauren Perrier Champagne. Oh, that's cheating! Yes! I win again. Which, you know, a machine couldn't have cheated, so I guess the humans have the advantage of after all. Special thanks to Keely Crockett and Paul Seager for making time for us this week. You can find links to everything we've talked about here on our website. Head over to theguardian.com forward slash podcasts. I'm Grey Jackson. I'm Jordan Erica Webber. And the producer was Danielle Stephens. Until next time, goodbye. Goodbye. For more great podcasts from The Guardian, just go to theguardian.com slash podcasts.