 The Guardian. There was a picture of, like, an upper girl's skirt, but it was probably, like, only that far from her crotch, you know, really right between her legs. And I just knew it was me immediately, because of how they were, and I knew what I was wearing, and I saw it, and I was like, that's me, and I grabbed the phone off him, and I, like, held it up, and I was like, you've been taking pictures of my skirt, what is wrong with you? And I was crying. In June 2018, the British government signed off on a plan to introduce legislation to ban upskirting, the practice of using a camera, on a mobile phone or otherwise, to take a photo up a person's skirt, without their knowledge, and therefore, without their consent. While it seems obvious that a nonconsensual act like this should be illegal, it has taken a campaign and some debate in the Houses of Parliament to get the government to agree to legislate against it. But upskirting isn't the only disturbing act enabled or augmented by advancements in technology. Anyone who keeps track of current affairs will probably be sick of hearing the words fake news, but the concept extends beyond articles written by trolls for suspect publications. One way to spread fake news is to manipulate video footage to make it seem as if someone, usually a prominent figure, has said or done something they never did. And, of course, it didn't take long for people to start using the same method to create deepfake pornography. The usual targets for deepfake pornography are celebrities, but some fear this could become a common weapon in the existing practice of revenge porn, and researchers are already trying to find ways to fight back. As with so many of the topics we've covered on this podcast, the question is, how do we legislate against a technology that is developing more quickly than lawmakers' understanding? In England and Wales, we've got a bit of a tendency to be really pedantic in our drafting of legislation, to try to narrow it down to very specific circumstances. But what happens is then technology changes, exploitation changes, and the new scenario doesn't fit with the old law. And considering how long the legislative process can take, is there a technological solution to combat deepfake porn? Unfortunately, we work in this area called digital media forensics. We're actually playing this, I will say, imbalanced cat and mouse game. I'm Jordan Erica Webber, and this is Chips with Everything. Hello. Hi, Claire. Yes, hello. Hi, this is Jordan from The Guardian. Hi, nice to speak to you. Yeah, good to speak to you. Claire McGlynn is a professor of law at Durham University. My main areas of research are around violence against women, particularly around online abuse, image-based sexual abuse, and the regulation of pornography. So we're talking about... Along with her colleague Erica Rackley, she has been fighting for better legislation in the area of revenge porn for a long time. So revenge pornography generally understood as sharing an intimate sexual image without someone's consent, often by, say, a malicious ex-partner. You know, that sort of harm and abuse is as old as it comes, but obviously with the internet, it's become much more prevalent over recent years. In 2015, in England and Wales, legislation was adopted to make it a criminal offence to distribute such an image without consent, but it is quite limited. And, for example, one of the things it does not cover is images that have been altered to make them pornographic, and that's where the controversy around deepfake pornography comes in. Revenge porn isn't exactly new, but this kind of deepfake pornography is. I wanted to know why this latest trend has sparked such fear among the public. The change now in terms of public debate is I think that people have suddenly begun to realise that it's actually getting easier and easier to alter images, to make them pornographic, and what we're seeing is this being done more and more often, and people are beginning to come forward and talk about the harms they're facing. So it's been an issue for a number of years. Some of us have been suggesting we need the change, and it's just not happened yet. Upskirting was made illegal in Scotland almost 10 years ago. In England and Wales, it's more difficult. The law in Scotland is different, so when they introduced their laws against what some people would call revenge porn a few years ago, they did cover images that are altered to make them pornographic. So England and Wales were just behind the curve at the moment. Do you have any thoughts on why? When the law was introduced in 2015, the public debate was very limited. It was focused just around this kind of classic case of a malicious ex-boyfriend sharing images without consent, and it was very difficult to get any space in that public debate to talk about the other types of image-based sexual abuse. So we got a very welcome law, but it was limited. So my concerns at the moment is all the public debate is around upskirting, which is a real problem, and we must have laws against upskirting. But I think this is why we need to just take a little bit of a step back and look at the broader picture and see, let's now take the opportunity to reform this broader area of law, not just upskirting. Otherwise, you and I will be having another conversation in a few years about the need to change the law again. In May 2018 in London, a man was convicted for harassment, jailed for 16 weeks and ordered to pay Â£5,000 in compensation after he produced fake pornographic images of a female intern who rejected his advances. But even with the new General Data Protection Regulation, there are no explicit rules against deep fakes. So how far are we from having laws that adequately reflect the possible effects for victims of online sexual abuse? We're getting closer, but I think we've still got quite a way to go. My hope at the moment would be to draft what I would talk about as a comprehensive law that is quite straightforward and just talks about the nonconsensual taking or sharing of intimate images or threats to do so or altered images. In other words, I'd hope that that would cover all scenarios. In England and Wales, we've got a bit of a tendency to be really pedantic in our drafting of legislation to try to narrow it down to very specific circumstances. But what happens is then technology changes, exploitation changes and the new scenario doesn't fit with the old law. So if we took a comprehensive approach, we could hopefully future-proof the law and then we'd be very close to dealing with these issues. So we talk about the pace of technology on this podcast all the time and how ethics and the law struggle to keep up. Do you think that on top of having comprehensive law, the type you mentioned there, one way we could deal with these issues as quickly as is needed would be to have a specific group of technologists and ethicists who could mobilise at a moment's notice to judge when a case like deepfake monography comes to the fore? Why not? I'm sure that would help. Technology companies themselves, obviously, are the ones who are at the forefront of both knowing what's coming round the corner and also how to, you know, take steps to act and contravene these abuses. I think it's as much as well about our legislature being reactive. And in truth, it is at the moment. It is going to take some action around upskirting and it's perhaps just encouraging our lawmakers to be more flexible and react to changing circumstances. Sometimes changes... Before we started recording, Claire told me that, despite her vast knowledge on legislation around online sexual abuse, she wouldn't be able to provide details on the exact technology used to produce fake images or videos. I wondered if that was why legislation can be so slow, even for what seems very clearly to be criminal behaviour. Is it because the lawmakers just don't understand how these technologies work? Possibly. But I think that would be the case for lawmakers in a vast array of issues. You know, no member of parliament, no minister in the government, can expect to be an expert in every single area that they come across. So I guess people do rely on experts to be able to share their knowledge in a way that others can understand. Technology might be slightly different. Many of us have a kind of myopia and say, we don't understand it. I understand the effects of it. I just don't understand how it actually operates. I couldn't write a computer programme to design this to do it. But I guess I don't feel I need to know how to write the computer programme, to know how this is affecting victims and therefore how we need to take action against it. So we may be on our way to legislating against deep fake pornography, but if we stick with this approach, we could end up in a situation where we spend months or years trying to create new legislation for each new kind of online sexual abuse as it appears. Because, as we know, the technology isn't slowing down. After the break, we'll look at the technological approach. And hear from an academic who is attempting to combat deep fake videos by turning the tech on itself. I think the real big problem is not the fake news or fake media itself. It's actually the sort of reduction of trustfulness of anything for online media for the general public. Don't go anywhere, we'll be right back. From providing cancer-fighting drugs to smelling like decaying flesh, plants have an amazing and diverse set of talents. For me, I find them fascinating are the carnivorous plants, so like the Saracenias, the Venus fly traps as well, that, you know, plants have evolved to eat insects and mice and rats and bats and all sorts. I think that's fascinating. Plants support our lives in so many ways, from sequestering carbon from the atmosphere to providing us with the food we eat and even decorating our living rooms. We couldn't live without them. But do we owe some thanks to the diversity of plants we see today to dinosaurs? So there's been various ideas about how dinosaurs might have shaped plant evolution. And we know that dinosaurs and plants did interact as herbivorous dinosaurs ate plants. Join me, Nicola Davis, on the Guardian Science Weekly as I journey back through time at RHS Hampton Court Palace Flower Show to find out. Just search for Science Weekly on your favourite podcast app or head to theguardian.com forward slash podcasts. Welcome back to Tips With Everything. I'm Jordan Erica Webber. If you're like me and my producer, you've heard the phrase fake news bandied about so much by politicians and media outlets that it's begun to lose its meaning. Some argue that the public has become apathetic to the problem of fake news, a kind of learned helplessness where we feel like we can't do anything about it, so we just don't try. But researchers want to help in whatever way they can. My name is Sui Lu. I'm an associate professor at the computer science department at the College of Engineering and Applied Science at the University at Albany State University of New York. Sui and his colleagues Mingqing Chang and Yujin Li recently developed a method of detecting fake videos. Well, early this year, the emergence of this deep fake app caught my attention. Actually, it caught a lot of attention from the media, partly because the way fake videos can be generated so easily with this software, you know, people have seen some malicious use of this software for generating pornographic videos or actually potentially making videos that will alter people's view about certain events. I'm sort of foreseeing a large social impact of this line of technology. So as always, we do as researchers in digital media forensics, what we want to do is form a first line of defense here. So that's when I started thinking about, you know, if there is any technique that we can use to detect a video has been generated using this software. So these scientists have begun to notice fake videos popping up on YouTube. But how exactly did they go about figuring out how to expose them? So what we did is we actually not only we collected a lot of the deep fake generated videos on YouTube, we also generate, we get a copy of the deep fake software ourselves, we play with the software, we also augment it a little bit. So we have the ability of generating fake, deep fake videos ourselves. One day after most of us looking at the video, I always get a feeling that some part of the video is not right. And then at that moment, I realized all the faces synthesized by the software, they simply do not blink and that's not normal. And then we realized that there is a fundamental reason. That is because all these fake videos are generated by a deep learning AI algorithm. So the big the deep learning algorithm using similar things. So what they did is they actually find faces, say your target faces is person A, you give this algorithm many, many images of person A, possibly, you know, millions of that. And then the algorithm will starting to learn what looks like person A. Now, typically, we got images from internet and we got an image of person A on internet and possibly this person or somebody posting his images on internet, only select those good images. And good images typically means images with no eyes closed. So we go ahead and develop another algorithm that can detect a blinking eye in the video. And then whenever we see a video that eyes do not blink as much as we expected, then that video may be a fake one. So are you not worried then that people who see your research and want to create these videos will not just say make an algorithm that introduces blinking into the videos? Unfortunately, you know, we work in this area called digital media forensics. We're actually playing this, I will say, imbalanced cat and mouse game. Whenever we have a detection algorithm, we published in scientific articles and we want our peers and, you know, the general public to understand this technology. By the same time, I think the followers will also take advantage of this. On the other hand, what are we trying to do here is basically lowering the threshold so that, you know, someone who really want to make such a good fake media, they have to put in their time and resources. We want to stop those cases where, you know, someone barely know some computer technology, some machine learning basics with a computer, with tons of training data, they can generate some video and cause them some damages. So that, you know, reading this threshold and making the cost of making fake video higher so that, you know, we'll see a few of them. If we get to a point where only people with a lot of money and resources can make this kind of deep fake videos, do you think that will get rid of deep fake pornography, for instance? So revenge porn, it's often, you know, an ex-partner. If you get to a point where just the average guy can't really do this kind of thing anymore, do you think we'll get rid of that problem? Currently, we will only deter that or defer that. But I think the technology on the other side, on the synthesized side, also growing very fast. I mean, just look at the recent development in deep learning and computer vision. I'm predicting the technology at our disposal to generating realistic fake videos are getting better and better. And with an exponentially accelerating speed, better videos will be generated by ordinary person. But on the other hand, I also have confidence that our forensic technology can also improve so that, you know, this will form a balance, will form a line, where, you know, it's sort of the entrance criteria for somebody who can make such fake videos. So despite the progress these three scientists made in figuring out one way to expose fake videos, the lead researcher is well aware that this type of research cannot keep up with the technology it's trying to catch. So what else can be done to combat deep fake tech? I think that we have, as a researcher, we have several different levels of tools at our hands to solve this problem, i.e. segment them into signal level detection, physics-based detection, semantic level detection, and the physiological-based detection. Now, signal level just means that you take the digital media as a digital signal and use all the signal-precising and machine learning techniques to find any difference in the signals. So for instance, if the fake video was actually generated by a particular software, video compression software, those compression algorithms will leave some characteristics. If we detect that, we can say, you know, this video is not authentic. Now, physics-based is more complicated. It requires us to actually see the inconsistency in lighting, for instance. Semantic level, it just means that the video is not taken at a time and a place where it's supposed to. That requires a lot of cross-linking of the visual media to the other tech media, for instance. And then what we are trying to do, I think, is a new research direction, is this physiological-based detection, where we're looking for physiological signals, like eye blinking, breathing, heart beating. So I think this four-level detection techniques will see a lot of development in the coming years. Why is it so important to find ways to expose technology like this that's used to distort the truth? I think the real big problem is not the fake news or fake media itself. It's actually the sort of reduction of trustfulness of anything for online media for the general public. So I think there has to be some regulation measures, there'll be some public education measures to do this. But I think from the technology and the research point of view, the whole technology community should come up with solutions to help the general public, the news agencies, or commercial companies to fight back this problem. I think it's becoming a serious social concern in the coming years. Before we sign off, we're heading to Uganda, where Parliament has imposed a tax of 200 Uganda shilling, roughly four pence or five US cents, on the use of social media. The change was approved in May after the President argued that social media encourages gossip. Critics argue that this move is an attempt to restrict criticism of the government, but some people have already found a way to avoid paying the tax by using virtual private networks. Will this catch on in other countries? Will it even work as a deterrent? Let us know what you think by emailing chipspodcast at theguardian.com. I'd like to thank Professor Claire McGlynn and Professor Sui Liu for joining me this week. There will be a link to Professor Liu's work on detecting fake news in this week's episode description on The Guardian website. That's it for this week. I'm Jordan Erica Webber. Thanks for listening.