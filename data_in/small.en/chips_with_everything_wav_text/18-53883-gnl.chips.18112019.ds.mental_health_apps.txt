 The Guardian. Just a heads up, this episode of Chips with Everything talks about issues of mental health, including suicide and self-harm. The UK is experiencing a mental health crisis. According to Mental Health First Aid England, one in four people experience mental health issues each year. But a 2018 study by the Royal College of Psychiatrists found that of the 500 diagnosed mental health patients surveyed, some had waited for up to 13 years for treatment. MHFA England also points out that while 75% of mental illness, excluding dementia, starts before age 18, about 20% of young people with mental ill health wait more than six months to receive care from a specialist. So we know there's a problem, and not enough is being done to fix it. As is often the case, the tech industry thinks it can help. A flurry of mental health apps, chatbots and virtual therapists have hit the market in the last few years, offering to provide help whenever and wherever you need it. And they're proving popular with users. But there are those who worry about depending on an app for the kind of help that has traditionally been provided by a human. It's the idea that somehow this can be a substitute for face-to-face or other forms of direct contact with a human being. Because what research we do have points to the fact that psychotherapies work because of the quality of the therapeutic relationship. I'm Jordan Erica Webber, and this week I look into whether technology has a place when it comes to treating mental health issues. This is Chips With Everything. Amar Kalia wrote a story for The Guardian about the rise of mental health apps. Mental health has become probably one of the biggest issues in terms of healthcare at the moment in the UK and globally. And in order to deal with that, the NHS and also the private sector have been producing and commissioning apps for therapy. So these are kind of phone-based or computer apps that young people and adults use to kind of work through their mental health problems. There's apps being developed which are using AI chatbots. So there isn't a human person on the other end of the app. It's essentially a program that you download on your phone and you will type into it, kind of saying that, oh, I'm feeling sad. And then they'll reply with a set of different responses. And that's where you can talk through your issue. The other way the tech industry is responding is to having person-centric apps. So they'll have a therapist on the other end of the app and you'll be actually typing and speaking to a trained professional. But both ways, it's just a kind of digitization of the sort of therapy experience. And the mental health tech industry is apparently very lucrative. Some of them are obviously being commissioned by the NHS. So in terms of the user, they're free to download. But there was a rough figure of $1.35 billion, which is the for-profit value of apps. I think that was in 2017. So yeah, massively profitable. You've already mentioned some of the kinds of apps that you talk about in the article. You spoke to a teenage boy who uses a mindfulness app called Cooth, who says that there is less stigma attached to using these kinds of apps. What kind of stigma does he mean? I think the stigma associated is, A, between having to tell people that you're going and getting therapy and B, I think the physical act of actually seeing someone and facing them and telling them a problem. So for the young people that I spoke to for this piece, what seemed really clear is that these apps provide a way to anonymize your therapy by means of not having to tell your friends and family, but also not having to even necessarily face a therapist or professional you can do it all on the computer over the phone. Sarah Niblock of the UK Council for Psychotherapy thinks that one reason people might find the idea of face-to-face therapy daunting is how it's portrayed in the media. Many therapists work beside a child or a young person or adult. They're not necessarily face-to-face in a room with a box of tissues. So there's a lot of popular myths and I think it's through films and TV. It can look a bit scary. You know, you think, oh, I've got to be really, really ill to go and see a psychotherapist. So I think there's a bit of that that might put people off. Certainly we would say, well, what is it about talking to somebody that you're concerned about? Because actually it might be pointing to the fact you really do need to talk to a person. So what's your main argument against these apps? I wouldn't say we're against apps per se. We don't take a sort of totalitarian view. In fact, some of our members are actually using technology in their therapies. It's more that we just don't know enough at this stage about the ethical implications of using technology. And we really don't know how these apps go down with clients. We don't know their benefits. We don't know enough about the opportunities or risks at this stage. And clearly we don't know where those apps are directing people. How can an algorithm really account for the complexity of every individual's human experience? If you're working with a therapist, they're working with you and you drive the process. And your lived experience, your history, your events, your feelings are unique to you and your relationship with the therapist is unique. An app cannot mirror that. An app is obviously going to have a limitation. And we'd be very worried to know whether they are misdiagnosing, whether there's a possibility they could be directing people in the wrong directions. But some developers make it clear they want people to use their apps in conjunction with traditional therapy. The idea came from the clinical work I do as part of my mental health role, where we were seeing many more young people coming through our service who were self-harming. And when I had conversations with these young people, they were self-harming. They were typically doing it on their own, but they all had access to a mobile phone at the time they were harming themselves. So we had some discussions with young people about whether it might be possible to look at putting something on their phones that might help them to ride out their distress and to manage some of their self-harm. Paul Stalard is a clinical psychologist at the University of Bath and is one of the developers of the app Blue Ice, a name that combines the notion of feeling blue with the acronym for in case of emergency, ICE. So the app provides a collection of tools, ideas which the young person can use to ride out any distress which might lead them to self-harm. And the idea is that each one of these sections can be personalised with the things which make a difference to that young person. So for example, there's a section in there about getting yourself busy, because we know that if you get busy, that sort of behavioural activation is a way of riding out and managing some of those sort of low mood feelings. But rather than sort of give them a list of where you could go for a run or you could go for a bicycle ride or you could go for a swim, the idea is that the young person populates it with the things which they do. There's sections on the app for changing the mood, so finding some of the sort of things which just put a smile on your face. There's a section on there which has got a sort of library of pictures, your photo library where you can upload some of these sort of good memories. There's a feel-good music library where you can put in some music which has got some very positive sort of memories with you, makes you feel good. And then there is the last two sections and one of those is about nominating three people who you could call if you're feeling particularly down. And these aren't people to call to say I'm phoning you because I'm thinking of harming myself, but it could be about people who they like talking to who always seem to make them laugh, who are just good people to be around who have a positive effect on their mood. And the final section is one on some ideas about how they can ride out their distress, so some sort of techniques about a sort of soothing toolbox which they could develop. So the app includes this feature where you encourage users to maybe get in touch with people in their lives. But what about professional human interaction? Is the app meant to be used in conjunction with real human therapy or is it kind of more standalone? Well at the moment the way we're using it is that it's being used alongside the young person's face-to-face mental health intervention. So there is going to be some sort of face-to-face support and we're quite keen on that at the moment because some of these people could start to become quite suicidal in their thinking and we want to make sure that they're safe and that somebody is keeping an eye on them and can respond and reassess their risk if needs be. Does the app also include information about emergency services that the user might also need? Yes it does. So that if the young person has got automatically routed into this mood lifting section, at the end of the mood lifter they will be asked how they're now feeling. And if the young person's mood has not changed they will be then asked a number of questions about when you felt like this last time, what did you do to help yourself, are you on your own, if you are on your own is there someone else you could go and meet with, is there somebody you would like to contact and then it takes them to the final screen where there are three emergency numbers which they can press to connect to 111, child line or a nominated person. Given the vulnerability of the target audience for the app and since we are a tech show we have to ask who gets access to the user's data? Well the app itself sits on the young person's phone, it's password protected, the young person sets up a password for that but the data which they enter is not transmitted anywhere so it doesn't get transmitted to their health clinician, it doesn't get transmitted to any website so the data they enter is theirs which sits on their phone for them to review at a date they wish to but the data isn't sent anywhere, it's not being transmitted, it's not being used for any other purposes other than what the young person puts it in their phone as a way of helping them understand and to identify some of their patterns. As part of testing the app, Paul and his team started a trial in September to determine whether or not Blue Ice did in fact reduce the number of children taken to A&E, accident and emergency, after self-harming. So what we wanted to do at the very beginning was to make sure that the app was safe and was doing no harm. So for example the app has a mood diary as part of one of the elements where young people can monitor how they're feeling. So we wanted to make sure that recording their mood as being low or down didn't have any perhaps negative effects on making them feel a little bit more hopeless or feeling helpless which might unintentionally have increased their self-harm. So we undertook a sort of study with these 44 young people who were using the app alongside their child and adolescent mental health face-to-face intervention so that we could keep an eye on them and what we did was that we asked them to use the app and we assessed them and we worked out with them about whether it made the self-harm any worse, whether they found the app useful, whether there were any adverse events and also whether it had any positive effect on how often they were self-harming and their mood. What's been the response from users? Surprisingly positive. I say surprisingly because I think adolescents are very critical of a lot of technology and I think that because we developed it with young people I think that the pitch is probably right and that young people have wanted to use it, have used it at times of distress and from our initial work what it appears is that almost three quarters of young people who used the app reported that it had helped them ride out some of their episodes of self-harm. So it didn't necessarily stop all the young people from self-harming but it prevented a number of those episodes from happening. Blue Ice is already being used by Child and Adolescent Mental Health Services, otherwise known as CAMS, and it's also part of the NHS Apps Library which aims to help the public find trusted health and wellbeing apps. But since the app is prescribed it currently needs human professionals to give young people access. So although we've got some very positive feedback so far, the study we're now doing is to prove with a little bit more certainty that there are going to be significant benefits that it is safe and that it doesn't seem to have any sort of adverse effects. And once we've been able to demonstrate that then our aim is to make the app more widely available so that young people can directly access it themselves. Sarah does see potential in apps like Blue Ice. Absolutely, and as I said I think that there's some great innovations in universities, scientists across the world are coming up with some fascinating opportunities and we are very much in support of that. And some of our therapists do already do that kind of work. These are good advances and with exponential changes in technology, great, let's go for that. But seemingly there's some advantage in people getting access to help maybe in the spur of the moment in an emergency. So maybe with young people who might end up harming themselves. There is absolutely vital need for young people to be able to access someone in the experience that they may be having of acute emotional distress and fear. But I really would be concerned if it was felt that this is somehow to be considered an effective solution to what is fundamentally a funding crisis which is not addressing a huge acute mental health emergency amongst children and young people. But some apps aim to offer an alternative to face-to-face therapy, letting people talk to a chatbot instead. I would like everybody to talk, but the reality is that there is a stigma attached to depression and other mental health problems. I would say that it's important that you find a solution that actually works for you. Whether that is the app in conjunction with the talk therapy or it's just the app or just the talk therapy, that's up to you. But we are trying to offer another alternative, a new tool in the toolbox for us. More on that after the break. Welcome back to Tips With Everything. I'm Jordan Erica Webber and this week I'm looking into the pros and cons of mental health apps. Before the break, the Guardian's Amar Kalya talk to us about apps that are meant to work alongside humans. But some don't. Instead, they offer interactions with a chatbot. The thing really to mention with the chatbot apps is that the makers of the apps really emphasize that it's really a first response kind of thing. It's not really meant for any serious issues. In that way, I think it combines quite a lot with the mindfulness stuff as well because it's all about small practical ways that you can deal with your stresses and anxieties. I think also another interesting point that was made, which was by an NHS commissioner, was that potentially these apps can get to BAME communities or other minority groups where potentially therapy is stigmatized. Then you can do it, I guess more secretly and more privately. So I think there's ways of getting to underrepresented communities and helping them out. Some people believe that by combining various forms of technology, they can offer an alternative to the traditional medical approach. My name is Daniel Manson. I'm a clinical psychologist and I'm the CEO and co-founder of Flow Neuroscience. Flow consists of a brain simulation headset and a chatbot therapist app. The headset costs £399, so won't be accessible to everyone who might want to try it. But the app is free and can be used without the headset. It will talk to you, ask you about things. It will guide you on what depression is, but also how you can reduce it yourself through lifestyle changes. We have focused in on the four pillars of exercise, meditation, sleep and nutrition. So it's basically you go into a session, you answer a couple of questions, the chatbot will talk to you and talk to you about that specific topic of the day. Just like I wanted to know from Paul Stalard earlier on in the show, I asked Daniel about where the data collected from the app goes. Their privacy policy states that, quote, no third party has access to your personal information as long as the law does not require us to share it. Having said that, Daniel didn't tell us exactly who within the company might have access to it. But there are very few people in the company who has the administrator access to the database. So after looking at the privacy settings and reading through a list of precautions, mainly for the headset, users see a menu of sessions, which at least for now, must be completed in a specific order. Right now, yes, you need to complete them in order. We had the idea from the beginning to have a full package, so to speak, when it comes to this. So we are giving them all of these four pillars that I mentioned before. We are now actually developing a new version where you have the ability to choose more because we realized that people made us aware that they could be very good at exercising, for example, but nutrition wise, they didn't know or meditation, they didn't do. So after Christmas, there will be a version where you can actually choose your path yourself. I can see how some people might try to go through the sessions as quickly as possible, you know, kind of complete the app as quickly as possible. Do you try to discourage that in the app? Not actively discourage it, but if you look at the actual how the content is written, it's written by clinical psychologists and we know that it's very difficult to get anybody depressed or not to do something actually in the real world. So it's written with all the psychological knowledge that we have about how to get people to actually do something because that's the most important. That's also why we haven't included a bunch of other pillars, so to speak, because we have tried to keep it simple and make people actually do it. So this chatbot will talk to you about food, exercise, meditation, things like that. Does it do any kind of talk therapy though or is it not quite advanced enough to do that? It doesn't do any cognitive changes. It doesn't try to change the way you think about your emotional life, for example. It is based on a therapy called behavioral therapy, which means that we are also helping them how to maintain a lifestyle change that they have done. So that's a kind of a more psychological approach to this, but it's not super advanced on the emotional part now. So when it comes to using the app itself, would you say that talking to this chatbot therapist should also be done in conjunction with talking to a human therapist? Well, it depends. Many people say that finally I've gotten an alternative to actually talking. As a clinical psychologist, I don't like to hear that because I would like everybody to talk, but the reality is that there is a stigma attached to depression and other mental health problems. I would say that it's important that you find a solution that actually works for you, whether that is the app in conjunction with the talk therapy or it's just the app or just the talk therapy. That's up to you. But we are trying to offer another alternative, a new tool in the toolbox, so to speak. How does the bot choose what to say when people are talking to it? Right now it's preset, so you have maybe three or four alternatives to choose from. And all of these have been written so that people can choose themselves and there will be a set answer for every path that you take. As Ammar Kahlia explains, these presets, in other words, the algorithms behind the apps, can be problematic. So there was a sort of small investigation that the BBC did with two apps called Wycen Wobot and with Wobot they sort of pose as a 12-year-old who'd been a victim of some kind of sexual harassment and they put that scenario to the chatbot and the chatbot replied by saying, sorry you're going through this but it also shows me how much you care about connection and that's really kind of beautiful, which obviously is a really inappropriate thing to say and I think when they came back and sort of pushed back saying that wasn't right, it just came with a kind of generic refrain saying, would you like to rewrite your thoughts? And I think yeah, that hammers home the point that you can't really go to these apps with any kind of serious issues. In response to the BBC investigation, Wobot did warn that it could not help with abusive relationships. And to be clear, Flow hasn't had problems like this. As Ammar points out, chatbot apps are perhaps more suited to people who are experiencing less severe issues. But it does raise the question of how we determine which apps are appropriate for which scenarios. Daniel says that Flow only aims to deal with clinical depression. I don't think it's an inherently human problem. I think it's a disease and something that should be treated. So what we are doing is that we are selling and developing a treatment for depression, clinical depression, that can be used together with other treatments or as a monotherapy. And that's basically what we want to do. We don't have any views on that, actually. We're trying to provide what we can in order to reduce this disease. Sarah Niblock of the UK Council for Psychotherapy isn't convinced a virtual therapist can be as effective as a human. Again, that's only going to really handle symptoms. So it's about managing behaviours. It's about recognising particular symptoms and maybe triggers. But it's not really going to get to the very essence of where those come from. It is simply just papering over the cracks and helping people maybe just get on with their day. But that's only going to take you so far. Daniel acknowledges that the Flow app might simply prolong those who need face-to-face therapy from seeking it, but argues that what they are doing is providing an option for those who decide they don't want human intervention. Just as with the antidepressants that came forward, we have a huge problem. So we need other solutions. I don't think that psychologists ever will become obsolete. They will just focus on maybe harder problems than the mild and moderate depression or the mild and moderate anxiety. I don't think they will have to be worried at all. My colleagues are not worried. They have other stuff to do. So I think this will be actually a tool for them to help them to take on even more patients. So, Sarah, what would you like to see happen in this field? Do you see a place for mental health apps in the lives of people with depression or anxiety? Absolutely. But it needs to be properly researched. It needs to be properly ethically frameworked. And I think there needs to be much more joined up conversation, a bio-social psycho model with a view to getting much more funding for research into these areas. But really, we do need to look at having a better understanding about the human mind and how people develop and become who they are. So I would like to think that technology will also be looking from that perspective and talking to organizations like ours, unto psychotherapists and getting their input into the creation of some of these devices. Thanks to all of our guests this week. Ammar Kahlia, Sarah Niblock, Paul Stallard and Daniel Manson. For anyone affected by anything mentioned in this episode, in the UK and Ireland, Samaritans can be contacted on 116 123 or email joejo at Samaritans.org or at Samaritans.ie. In the US, the National Suicide Prevention Lifeline is 1 800 273 8255. In Australia, the Crisis Support Service Lifeline is 13 11 14. Other international helplines can be found at www.befrienders.org. That's all from me this week. Chips is produced by Danielle Stephens. I'm Jordan Erica Webber. Thanks for listening. For more great podcasts from The Guardian, just go to theguardian.com slash podcasts.