 The Guardian. So I think it's fair to be worried about AI, so I wouldn't say we are just being optimistic about it. But we want to be thoughtful about it. AI holds the potential for some of the biggest advances we are going to see. At the end of March, Google launched a global council that would advise on ethical issues around artificial intelligence and other emerging technologies. They called it the Advanced Technology External Advisory Council. A week later... Google is under fire again for pulling the plug on its AI ethics board. They announced they were shutting it down. A group of Google employees had protested the inclusion of what they considered to be a right-wing think tank leader and had called for her removal because of previous remarks she had made that were thought to be anti-LGBT and anti-immigrant. Another member of the council had already resigned. In a statement, Google said, It's become clear that in the current environment, A-T-E-A-C can't function as we want it. So we're ending the council and going back to the drawing board. This got me thinking of an episode we did in the summer of 2018 when Google's CEO Sundar Pichai wrote a blog post outlining seven objectives the company had going forward for working with AI. At Chipps, we're constantly looking at how big tech is attempting to grapple with the ethics of the AI they're making. So this week, we're going to replay that episode to refresh our memories about what Google had hoped to achieve when they first set out these principles. Don't worry, we'll be back next week with a new episode of Chipps. I'm Jordan Erica Webber, and this is Chipps with Everything. Sorry, I didn't realise. That's okay. I didn't realise either. That's okay. This week, we've brought together some of the best minds across tech and ethics to discuss ethical standards for the research and development of AI. So this week, I'm joined in studio by Sanjay Modgill, a senior lecturer from the reasoning and planning group at King's College London, Dr Yasmin Erdan, a senior lecturer of philosophy at St Mary's University in London, and on Skype, we're joined by Dr Sandra Wachter, research fellow in data ethics, AI, robotics and internet regulation, cyber security at the Oxford Internet Institute and the Alan Turing Institute in London. So that's three people who are incredibly qualified to help us discuss this week's topic. I should also point out that we did reach out to DeepMind to see if someone from the company wanted to discuss this with us, but they weren't available at this time. So, Google's CEO Sundar Pichai, or at least someone from his office, published a blog post in early June that outlined Google's seven principles for working with AI plus four areas in which they will not use AI. You've all had a look at them, and we'll get into the specific commandments that intrigue you in particular later on. But first, Sandra, just from you, could we get some context for the conversation? So, just could you remind us of Google's history with AI? Yes, so I think Google in particular, but actually all major tech companies have become very aware of the ethical implications of AI. And for quite some time, I think everybody was very keen on finding a way forward to at one hand harness the full potential of AI, but at the same time find solutions to mitigate those risks. And I think the statement now is a very good step forward to actually pledge that some ethical consideration will be at the heart of their work. I'm interested in why Google is doing this now. Have they got into trouble before for the AI research, anyone? Well, it does seem as if the staff rather rejected their work on military drones, and I think that that puts enormous pressure on them to do something. Is that Project Maven? That's right. I think also there has been some talk recently about the fact that the don't do evil principle has been quietly dropped from their commitments. Don't be evil had long been Google's unofficial company motto, but when the company was reorganized under its parent company Alphabet, it was slightly adjusted to do the right thing. So, Yasmin, it's not the first time that rules or principles have been written up by organizations that are concerned with using AI responsibly. In fact, the Future of Life Institute, for example, has had hundreds of AI researchers sign its list of AI principles, including many Google employees. So, why did Sundar Pichai decide to write his own list, and why right now? I mean, it seems like a very good step forward. I can't help but be slightly cynical, and it does seem to me rather more of a PR sort of offensive than much else. For instance, I'm quite concerned they're called objectives, which doesn't suggest to me an intention to make this, you know, something concrete that you must abide by in the same way that something like rules or laws or principles, as you described, would suggest. I think they do come across as rather vague, and of course the devil is always in the detail. But I do think it's a step in the right direction. Let's hope that there are some follow-up work on these principles. They're made more concrete, that there does prompt a discussion as to what the appropriate regulations are. And I think we really have to see now how these principles are now fleshed out in a bit more detail. The very vagueness of the principles means that you might follow them and inadvertently that may still lead to harmful consequences. I mean, in some sense, this is the rhetorical thrust, ironically, of Asimov's three laws. If we encode these kinds of principles in the AIs themselves, they may still eventually cause harm, even though they abide by those principles in their general sense. Before we started recording, I asked each of my guests to pick one of these seven objectives that stood out to them as particularly interesting. So we're going to dig into some of these principles in more detail, and we'll start with you, Sanjay, of these seven commandments, or objectives, as they're called, laid out by Pichai. Which one in particular intrigued you? I started with the one on being built and tested for safety. So of course there's a major issue and a major concern about the safety of artificial intelligence. What interested me was many of these doomsday scenarios have anticipated problems arising when these artificial intelligence technologies are developed by the companies. So what strikes me as being a difficult issue would be when these artificial intelligence machines start to be used by Google DeepMind, they start to become more autonomous. They start to engage in research and development themselves. And then we might have the problem that the actions of these machines will not be aligned with human values. So that goal is a very complex goal to unpack and to make sure that it is actually satisfied. So that's principle number three, the built and tested for safety. And I'll just elaborate on what Pichai writes there. He says, we will continue to develop and apply strong safety and security practices to avoid unintended results that create risks of harm. We will design our AI systems to be appropriately cautious and seek to develop them in accordance with best practices in AI safety research. In appropriate cases, we will test AI technologies in constrained environments and monitor their operation after deployment. Yasmin, it seemed like you had something you wanted to say there. Well, I think one of the problems there is, again, in this terminology that they're adopting, they talk about being appropriately cautious. And I mean, that leaves so much room for ambiguity and what we think about as appropriate and in what context and according to what needs and interests. Also, to be cautious doesn't mean that they don't do something. You can still do something and be cautious about it at the same time. So it doesn't really commit them to all that much. And I have the same sort of concerns with a lot of these objectives. But when it comes to safety and security, that's really a very big problem. Yeah. Does anyone have any thoughts for how you actually find out if an AI is safe? Maybe it's not so much a question of finding out how an AI is safe, but ensuring that an AI is safe in the sense that in order to make an appropriate ethical decision, one would require input from the humans. I quite like the idea that they had, I mean, I know these are very, very, very general, but they have principle four, be accountable to people. Now, I think the suggestion there, that's very much linked to this issue, which is that these ethical decisions will need to have input from the humans on an ongoing basis, because of course our moral and ethical systems evolve over time. It's not like you can prescribe something from the outset and then let it go and assume it will be appropriate. Does anyone have any thoughts on whether this will affect things like autonomous vehicles? So the idea that something has to be tested for safety, if it has the potential to kill people, does that mean you just shouldn't work on it at all? I think there is, what's very important to distinguish is research versus deployment, right? I think those things are very often mixed together and that's not actually accurate. You can research and test a lot of things before you make the decision to deploy it. So when it comes to critical safety systems like autonomous cars, I don't think anybody would argue that we shouldn't be testing them. In fact, everybody would agree that we should test it in a lot of different and diverse environments to make them learn from the mistakes and stuff like that. The other question is, when is a product safe enough to be released to the market? And I think this is where we probably, this is where the regulation stuff starts. Not so much with the research actually. Do you think that an outside body should regulate the safety of AI at Google? I don't think there is like a one fits all solution to that. I guess there are algorithms that have not so many risks attached to them, where we might not need oversight, but for certain areas, I think it might be irresponsible to not have oversight. I mean, we have cars, right? We have public bodies regulating how to use cars, and we need to get certificates for being allowed to use them on the street, because there are risks attached to it. So if there is a deployment or a system that has similar issues, then yes, of course we need oversight from a governmental or independent body. Yasmin, you mentioned Project Maven. If Google was following this principle of making sure things are tested for safety, do you think they never would have been able to pick up that contract in the first place? Well, I think in a way, they've sort of left space to do that because they've said that they're not going to be involved with military equipment per se, but they did say they would still be working with governments, and they gave a list of ways in which that might play out. So again, I'm not sure that any of these objectives present a particular barrier to that sort of work. Of course, an interesting thing would be, well, what if Google were involved in, let's say, the development of a certain kind of sensor technology that was extremely accurate at identifying an ISIS terrorist? Now, that sensor technology could be used by an autonomous weapon, which was not developed by Google, but it still makes crucial use of a technology that's been developed by Google. Now, maybe this principle allows them, gives them the leeway to develop such a sensor technology, and one furthermore could argue that on a utilitarian ground, that actually on balance that would be a good thing. What actually would it be detecting when you say a terrorist? I mean, it's a thought experiment, if you like. I mean, I'm a philosopher, so I love thought experiments, but I would also say that there is a real problem with them. I think that they sometimes seduce us into thinking that a scenario that could, in principle, at least in our minds, be real, therefore has some possibility in reality, and the risk with that is if things happen as a result of that possibility. So I think we need to be really careful about either suggesting what AI could do in a way that actually is not going to happen or suggesting what it could do, and therefore we need to take account of that in terms of what we think about ethically and in legal terms. So I'm not really sure how technology resolves the problem, which makes good use, or bad use, if you prefer, of the sorts of problems that we have in our human engagement and then feeds them into technology. So it's difficult to envisage exactly how Google will make sure the AIs they deploy are fully safe, but a plan is in place, and as Sandra explains, the blog post is symbolic. It represents the beginning of an important conversation. After the break, we'll discuss Google's ongoing issue with unfair biases in their AI software and what it means to be accountable to people when working with AIs. Up until this point, from a legal perspective, there wasn't much that you could actually do because we really don't have any laws governing AI. We'll be right back. The Voice Lab, from The Guardian. Hey, do you ever want a quick catch-up on the news headlines first thing in the morning while you're making breakfast or getting dressed? Well, if you have a Google Assistant or Google Home, we can help with that. The Guardian briefing is an experiment from The Voice Lab. The voice is a very simple and easy way to get people to know you. It's a very simple way to get people to know you. The Guardian briefing is an experiment from The Voice Lab, which in under two minutes brings you up to speed with what you need to know about the day's top stories. We'll make sure you don't miss a thing. To listen at any time, just say, hey Google, speak to The Guardian briefing. Welcome back to Tips With Everything. I'm Jordan Erica Webber. Before the break, I and a small group of experts started to dissect a recent blog post from Google's CEO Sundar Pichai, which outlined the technology company's seven objectives for responsibly working with AIs. It seems pretty obvious that AI can reinforce bias in some pretty horrific ways. Just this week, my friend Shella, who actually co-hosted this podcast for our Black History Month episode, tweeted that Samsung had created a storyline from her photos called My Furry Friend that included photos she'd taken of animals and one picture of her with her hair out. Now Google has also made some serious mistakes on this kind of thing, but do you have any ideas how Samsung, Google and everyone else can actually avoid reinforcing unfair bias? I'm actually not at all surprised by that. I'm horrified by it, but I'm not at all surprised. And the reason I'm not is because of the scandal in January of this year when Google facial recognition, image recognition, identified human beings as gorillas. And their response to that particularly struck me, because what they did very quickly was to remove the use of the term gorilla in that image recognition software. Now that doesn't get to the problem of what that software is doing and the biases that it's replicating. I think that for as long as they keep looking for these post hoc solutions that don't really tackle why the problems end up in the software, then it's just going to repeat itself. What do you think Sanjay? Do you think it's possible to avoid bias with this kind of tech? It's going to be very difficult. I mean, we've already seen as we've seen many examples of bias and in some sense, the data of course includes the bias and the predominant paradigm in artificial intelligence is machine learning, which is essentially learning from precedent, from the data. And I think this is behind principle four, which is corrections of these biases will require that the machines explain their reasoning and make their reasoning transparent. And my understanding, the hope is that they are aware that it's a problem and they are working on solutions, albeit they may not have developed particularly sophisticated solutions and in particular the problem of course is with explaining the reasoning. But that's my understanding of how they're looking to address this issue. What strikes me is that it is phrased as we should, you know, minimize biases. I think it would be smart to say we should increase diversity. And I think that would be actually helping the problem because the problems of bias and discrimination is partly because the data is biased and it just replicates the stereotypes that we have. We also have a problem in the coding community in terms of biases because the coding community is not very diverse. So obviously those people have intentional and unintentional and latent biases. They are just going to, you know, everybody has certain convictions and this will be fed into the algorithm as well. And the last thing is diversity in terms of disciplines. There is probably not just a tech solution to the problems that we're facing. We actually need different disciplines to think about this together. When you talk about the political impacts of AI, political scientists are important. Are you talking about the economic impacts of AI? You probably need economists for that. Is it about the legal implications? We need lawyers. Is it about the ethical implications? We need philosophers. So it's not just about tweaking the algorithm to make it, you know, seem unbiased. It's actually a holistic project that you need to embrace and that's diversity. I mean, perhaps, again, this will be a place where regulation could dictate that there is the kind of, for example, the required diversity in the bias correctors. You've got this, you know, a team of bias correctors that say, no, no, no, this is bias. And regulation could help there to say, okay, well, you need to make sure you've got a full, you know, representation of different cultures and genders, etc., etc. And that's where regulation could really make a difference. So our panel agrees that when it comes to AI's creating or reinforcing unfair bias, we can't solve the problem with more technology. We need to look to human beings. But what happens when issues do arise? Who is held accountable and how? Which brings us to Sandra's chosen objective. So the issue of accountability has come up quite a lot already in this discussion, but Sandra, you wanted to dig a little deeper into this, which is principle number four, which is be accountable to people. So Pichai writes, we will design AI systems that provide appropriate opportunities for feedback, relevant explanations and appeal. Our AI technologies will be subject to appropriate human direction and control. So up until this point, how has this kind of thing worked? If people had complaints about AI, how did Google deal with them? So far, there has been a very vivid discussion about accountability in general, because, you know, the algorithms are very unpredictable in what they do. So you don't really foresee how they're going to make decisions and what they're going to do. If they do it, you might not be able to understand it. And they work very unanimously. So it's basically the algorithm doing stuff and not necessarily human. And all those things are very often used to say, well, you know, it's not the human's fault. The algorithm did it. And that's a very bad, bad excuse. And I think an excuse that we should not accept in a society. The whole computer said no. And I don't know how it works. But let's just move on. Let's not try to open up the black box. It's a very big problem. Up until this point, from a legal perspective, there wasn't much that you could actually do, because we really don't have any laws governing AI at all. We do now have the new data protection framework in Europe. But it's a very good step forward in ensuring explainable and transparent AI and giving users or the general public a way to start engaging in dialogue rather than saying, this is a black box. We don't understand it. And that is the end of the discussion. And in fact, if you were to ask someone who was a researcher in machine learning, well, can you explain why your machine learning algorithm made the decision it did? They'd be flummoxed, frankly. So it's one of the most pressing problems. Yeah, it's still quite woolly. It offers a very promising intention. But yes, I certainly can't see what that concrete outcome could look like, especially when you take into account quite how many different problems may arise and do arise in response to these sorts of AI systems. Sandra, do you think that this goal would be better achieved if the tech industry as a whole just kind of came together and called for a regulatory body to specifically look at companies that create AIs? Yes, I think even though I think the first step forward was very good of Google to say what they think, how the industry should move forward, I think the next step is actually to start a conversation with the whole of industry. For example, the partnership on AI would be a very good place to have that. And I'm very much hope that this will actually start a fruitful conversation, not just with the tech industry, but also with civil society and academics and government around that. I'm not saying you should regulate everything, but to a certain extent, if we have real problems, then yes, we probably need some regulatory oversight for those things. That's all for today. Come back to us next week for a brand new episode. Chips is produced by Danielle Stephens. I'm Jordan Erica Webber. Thanks for listening. Thank you.