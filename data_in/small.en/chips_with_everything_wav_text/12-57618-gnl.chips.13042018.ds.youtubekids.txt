 The Guardian. These days, we're hearing a lot about algorithms. The word tends to crop up when some tech company or other is forced to apologise for whatever new scandal has thrown them into the spotlight. Mark Zuckerberg promising to fix Facebook by making changes to its newsfeed algorithm, the social media giant says the change will... And whether the issue is big data and profiling or search results and suggested content, it's the algorithm that gets the blame. Hey there, it's us again. This is my brother Peter, mom and dad, and I'm Dasha. Today, we're gonna be showing our friends Andrew and Lisa the basics of the internet, and we thought you might wanna come along. It'll be cool. Now here's a little background. When we installed internet access on our computer, I got... The latest victims are children, and the troublesome algorithms responsible may not be long for this world. At the beginning of April, Buzzfeed News reported that YouTube is planning to release a new version of its supposedly child-friendly platform, YouTube Kids, that bucks the trend of relying on algorithms, instead opting for curation by human beings. The reason? So to use some examples, Elsa, a character from Frozen has been urinated on where characters are taken to strip clubs, really inappropriate, really poor taste material for children. And the whole point is that those videos have been put onto YouTube Kids, they haven't been picked up by the algorithms, and that means that children will start to watch them, expecting they're seeing an episode of their favourite show, and instead of seeing things which no child should be seeing at their age. Quick, quick, quick. Mmm, soda. Quick, quick, quick, quick. So much soda. Ah, uh-oh. Hey, let me in, let me in. Oh, no. Pfft. Oh, that's better. Ah-oh, ah-oh. Oh, no. Despite parental controls and moderation by both humans and robots, child users of YouTube and YouTube Kids are able to follow links to videos that seem safe in the suggestions bar, but turn out to be inappropriate or even highly disturbing. These videos range from the surreal... The massive flying saucer came over, everything lit up, and the whole family was being abducted, it got real bizarre, but what's amazing to me is that, yeah, immediately after you see something... To the downright distressing, they can click on what looks like a fun video of a baby mini and Mickey Mouse playing hide and seek that then shows Mickey getting stuck in a swirling washing machine, his mother screaming. Yet more sinister videos even show things like children being forced to eat feces or popular cartoon characters killing themselves. So why is this happening? How do these algorithms work? Could scrapping algorithmically suggested content be the best way to protect these kids? If it is the case that in order to have a completely human curated YouTube Kids app, you have to have a vastly reduced array of content, is that a good trade off? Might there be an awful lot of content that children would quite like to see, but which there just simply won't be time for human moderators to watch? And are algorithms just a scapegoat for big tech companies who don't want their human representatives to take the blame? This is Tips With Everything. I'm Jordan Erica Webber. We're used to teaching kids about stranger danger, how to cross roads and not to run with scissors. But the internet has introduced a whole range of new dangers and parents and child safety organizations are struggling to keep up. Hi. Hello. Hi, Andy? Yes, hello. Nice to meet you. Andy Burrows is the associate head of Child Safety Online for the National Society for the Prevention of Cruelty to Children. He says that the challenges facing organizations like the NSPCC have changed significantly over time. Well, certainly what we are seeing is that the nature and the scale of the challenge around online safety is growing. And certainly as well, the nature of that threat is evolving. So a few years ago, we would have been looking primarily at abuse imagery. And now we're starting to see real risks around live streaming, around live video. So as technology is changing and evolving, so the risk is evolving as well. And I think there's a real challenge there for everyone who wants to keep children safe, which is about making sure that actually we can start to be a bit more ahead of the curve instead of having to rush to catch up with the risks as they emerge from developing technology. So you said the risk is kind of changing over time. What's the biggest risk at the moment, do you think? Well, our big focus at the moment is about the risks that are out there on social networks. We know that almost half of children under the age of 13 in the UK have a social network, despite the fact that you're supposed to be 13 years or above in order to hold one. And then the majority of children 13 or ever do have a social media account. And we're really concerned that, frankly, social networks are the wild west for children. There's huge risks stemming from bullying on one end of the spectrum through to significant risks around online grooming, around the potential for sexual abuse. And we are really concerned that for the last decade or so, we just don't think social networks have been doing enough. They haven't been taking the threat seriously. And that's why now we are calling on the UK government to step in. We think it's absolutely essential now that government steps in to provide proper regulation in this space to keep children safe. So we want specifically to keep kids away from spaces that are supposed to be occupied only by adults. But what about spaces that are designed for children? So companies like Facebook and YouTube have started creating these apps and platforms for kids, specifically targeted at them. So things like YouTube Kids and Facebook Messenger Kids. Do you think there are risks from these kinds of platforms as well? Well, we're supportive of steps that social networks have taken. And Facebook and YouTube are a couple of examples of platforms that have taken steps to create walled garden platforms, to create safer versions of their sites for children. That's a good thing insofar as pragmatically, we know that children do want to be on social networks. So it's a good thing if we're seeing sites take steps to create safer versions. But if the likes of YouTube are going to create sites that are designed, that are pitched as being family friendly, that are being described as being suitable for children from three years up is, I think, the description of YouTube Kids on the app store. It has to follow through. We have to see those platforms absolutely being safe. And unfortunately, we've seen with the example of YouTube Kids, time and again over the last few months, examples of where inappropriate and potentially dangerous content is on the site. And YouTube is not doing enough to police the site to make sure that children are only accessing the types of family friendly content which the site is supposed to offer. Have you got any specific examples of this kind of inappropriate content that kids are coming across because of these algorithms on YouTube? Have any parents come to you with any? We regularly hear from both parents and children who are concerned about the inappropriate content that they're seeing on sites like YouTube. There's been a number of high profile examples, many of which have been in the press over recent weeks and months. So to use some examples, examples of where Elsa character from Frozen has been urinated on, where characters are taken to strip clubs. Really inappropriate, really poor taste material for children. And the whole point is that those videos have been put onto YouTube Kids. They haven't been picked up by the algorithms. And that means that children will start to watch them expecting they're seeing an episode of their favorite show and instead of seeing things which no child should be seeing at their age. Is there anything you recommend for parents who are worried about this kind of thing? What should they do? Well, we know right now that even the walled garden versions of these sites still have content on them which is not suitable. And so the really important thing if you're a parent is to make sure that you're keeping an eye on what your child is doing, but also that you have really open and regular conversations with your child. So if your child then does come across something which is unsuitable or even worse, they come across, for example, someone who might be wanting to try to groom them or to make inappropriate contact with them, they know that they can come to you for help and for support. So really it's about the regular conversation so that you can build up a regular trusting conversation. If your kid is coming home from school every day, you'd ask them how their day went, ask them what they're doing online. Just make it a really regular part of the conversation that you have with your child. Have you seen any positive steps being taken to improve child safety online in terms of regulation or even technological improvements? Well, there are positive steps that are taken. In the case of YouTube, for example, you mentioned some of them. They have taken steps to try and demonetize some of the inappropriate content or to demonetize content which potentially could attract negative comments or potentially risky content. Those are good steps. We know there are features on other apps which are positive, algorithms being used to detect, for example, nudity or other potential risks. But what we need to see is a dramatic increase in resources and commitment to keeping children safe on these platforms. So it's really good that the likes of YouTube are scaling up the number of moderators. I think they've said there'll be about 10,000 moderators across Google as a company overall. We need to see as many moderators as possible to ensure that children are being kept safe. But most of all, we need to see consequences if the social network firms aren't prepared to take those actions. That's why I really think it's essential that we do have regulation in place that says sites have to take steps to keep their platform safe. They have to take steps to ensure that they are transparent about the scale of the risks on their sites so that all of us as consumers, as parents, as children can be aware of what some of these risks are. And there has to be material consequences if sites don't take these steps seriously. Because this Wild West has gone on for too long. It's absolutely vital now when we know that social networks are at the center of children's lives, that they really are safe. Do you think there is some potential way in future for there to be a platform that parents can 100% trust? In the first six months of a new offence of sending a sexual message to a child, there were over 1,300 cases of grooming on social network sites. And 2 thirds of those were on just three sites, on Facebook, on Snapchat, on Instagram. Now, the scale of that challenge is huge. The scale of that challenge is really significant. And it can't be the case that social networks continue to have bad news cycles. They have a bad day's headlines, and then we move on to the next story. We have to see, with child safety, material change. And that means that the social networks just have to clean up their act. And if they won't, then as I say, that's why we need government to step in. But it can happen. We can have social networks that are so much safer than what we see right now. So Andy thinks that governments need to step in and tech companies need to step up. After the break, we'll hear more about YouTube's rumoured plans to do just that. I think that the biggest difficulty, though, if you like, is human nature. A lot of these things work best if you assume that humans are not going to try and subvert these processes, that they're not going to try and put up material which is harmful or offensive or dangerous for children. They're not going to try and put up material which is of extremist or pornographic nature. We'll be right back. So my father coined the famous slogan, drop the gun, pick up the pen. And in Somali, that's... This is a quote that you can still see marked up on the ruined walls of Mokti-Shu. Ilweta has since returned to Somalia joining her mother to continue the work of her late father, who was murdered for his attempts to bring peace to the world of the world. Ilweta's mother, who was a woman who was a woman who was a woman who was a woman who was a woman who is French. And he has a strong taste for his attempts to bring peace to the country during the civil war. Till today, no one knows who exactly pulled the trigger. But it's common knowledge of who made the order. And this was a political assassination because of his command and ability to actually provide an alternative to those conflict. To have a listen head over to the Guardian.com forward slash podcasts or search small changes on your favorite podcast app. Welcome back to Chips with Everything. I'm Jordan Erica Webber. Before the break we heard from Andy Burrows about the problems facing children and their parents when it comes to watching content online, even on walled garden sites like YouTube Kids. It all goes back to this issue with algorithms. What is an algorithm? So at its simplest an algorithm is something that we're all very familiar with. A cooking recipe for example would be an algorithm. It's basically a set of instructions that tell a computer what to do and importantly also in what order. Dr. Vicky Nash is the Deputy Director and Policy and Research Fellow at the Oxford Internet Institute. If we think about how an algorithm would work in suggesting content there are two sort of you know key things it might contain. One might be so quite general rules about you know what type of content individuals under 13 might want to see. So for example there's bound to be certain rules for example about not including content which is tagged or has identifiably any sexual content or violent content for example within it. So there will be rules if you like in the machine code to explain that. Then there may also be elements which are about helping the algorithm or the computer provide content which is relevant and interesting to the individual. So in that case there'll probably be two things. One will be based on what the individual is currently watching. So if you've watched Peppa Pig it's more like to throw out more Peppa Pig for you next. And the second will also be obviously if there's a way of identifying you as a user with the same user on that device or machine on that location last week then obviously it will also use your background history as well again to throw up suggestions based on what you've watched in the past. I think perhaps an important distinction to make around this as well is whether or not the algorithm can learn itself. So is it programmed to look at your habits and modify the algorithm itself in the future? So does it modify the instructions going forward or if you like does it just stick with a sort of a set of instructions that are programmed into it from day one? My producer Danielle Stevens spoke to Dr. Nash to try and understand how children are being exposed to disturbing videos like the kinds I mentioned earlier. She started off by asking why these companies are targeting children in the first place. So that's a good question. So I would say there are two angles on this. One is and it depends on how cynical you are I suppose the first would be that this is pure corporate self-interest and that it makes sense to get younger users using these platforms as an early in age as possible and so you know if you can produce a platform which is relevant and attractive to younger users under under 13 you've got a better chance of sort of keeping them if you like when they are older than 13 and then of more value for advertising purposes. There's also though if you're maybe less cynical and have a more optimistic view of the world and I have to say I do buy into this there's also a very good reason for introducing these platforms which is that they are specially designed for younger users. They are therefore able to ensure that they offer the appropriate protections that the law requires for example about things like data collection or availability of advertising and also to ensure that the content provides on those platforms for younger users is suitable for their age range as opposed to sort of throwing them in if you like amongst all the adults on the mainstream platforms. So yeah two sides of it and I'm my suspicion would be obviously that in most cases they both go hand-in-hand. If we look at the idea of YouTube kids specifically it's basically the idea to have a safer space for children as you explained so there could be parental controls and it kind of controls what kids can see but there have been issues which is what we're looking at in this episode. It's not perfect. What has gone wrong with the platform? Right so my sense would be is that it's been quite heavily reliant on using algorithms or codes to suggest options for children to watch. In other words there's not been if you like a human being saying this channel is appropriate for children this one is not this program is good for children this one's not. So you know whenever you leave something to a computer to make a decision the chances are that there will be and actually much as there would be with humans in fact but there will be mistakes. Yeah I mean what are the risks to algorithmically suggested content? We see it popping up but can you just identify some of the risks? Sure so I suppose we see similar problems around things like filtering so it's it's a bit like sort of the unknown you know the problem that was always raised about having unknown unknowns so if you if you program an algorithm you might very well be able to program it to deliberately avoid material that contains a sexual content or violent content but you know can you think of all the other possible things that you wouldn't want children to see can you build that into your algorithm and can you ensure then it operates with a hundred percent accuracy. So and I think the some of the examples that have come out most recently were around things like conspiracy theories the idea that if you just typed if you look for videos on say UFOs or aliens in the YouTube kids app you might just not get cartoons or toys you might also get access to some stories explaining for example that you know humans are descended from aliens so you know conspiracy content and now you know that could be partly just that the algorithm wasn't tuned effectively enough that somebody had thought about that but but but not if you like designed an algorithm that could pick up on it but it could also just be simply that it's never really occurred to anybody that this might be problematic sort of content on this platform and that children might see it so it's it's a difficulty both as I say it's a difficulty both of programming it effectively but also ensuring you've identified all your unknown unknowns. It was reported at the start of April you kind of touched on it there although not confirmed that YouTube might be getting rid of algorithms altogether for the YouTube kids app how would this work? My sense would be that I mean it would probably have to restrict the availability of content within the YouTube kids app because you're not reliant anymore just on computer code to identify what shouldn't shouldn't be seen so it would be a bit like a white list I would imagine there'll be a much smaller amount of content but that content will have been either all watched by humans or will come from if you like perhaps perhaps even sort of you know certified providers you know be like branded channels so again branding is often a way to determine on things like YouTube generally whether something is going to be you know official content say from Disney or YouTube or or provided by user perhaps pirated perhaps more sinister so yeah I don't know yet whether it will be all watched by humans or whether there will be a mix of perhaps content that's been watched and also sort of reliable branded channels but that would be my presumption. Do you think something like this could work? I do think I certainly think it could work I think it would be interesting initially to have the two together because again just thinking about this from the perspective of children's you know what benefits children what children want to use and what to access if it is the case that in order to have a completely human curated YouTube kids app you have to have a vastly reduced array of content is that a good trade-off might there be an awful lot of content that children would quite like to see but which they just simply won't be time for human moderators to watch I you know I think it would probably provide extra reassurance there for parents and to that extent I think it probably is a very good development you know if it is genuinely coming. Algorithms have been getting a bad reputation at the minute not just at YouTube at different companies do you think big companies like YouTube or Facebook take enough responsibility for their algorithms or do we see kind of a tendency to dissociate from their algorithms almost like they're unruly teenagers that they can't really control? I do think these companies try and keep a really strong eye on their algorithms my understanding is that you know they're different algorithms that are used are being updated and redeveloped daily and of course it would be in their interest to do so to ensure that they provide services and channels that individuals want to use and safety as a part of that. I think that the the biggest difficulty though if you like is is human nature a lot of these things work best if you assume that humans are not going to try and subvert these processes that they're not going to try and put up material which is harmful or offensive or you know dangerous for children they're not going to try and put up material which is of extremist or pornographic nature and once you have determined people who wish to that it's very hard to design systems that will completely keep it down. So yes I do think that they are trying I sometimes think perhaps you know perhaps it's very hard for big companies like this to understand the sort of the scale of harms that may result even just a sort of single individuals and to anticipate you know how much that will matter both for those individuals but also as in the Facebook story perhaps you know sort of across society more widely once it becomes understood and known about. Do you think it's possible to ever get to a point where children can surf these kind of platforms and not be at risk of seeing distressing content or is that just the way the internet works? So I think in walled gardens which are you know as we were saying just now so entirely say not moderated if you like or guarded by humans I think that is possible but I would regard this a little bit like the way that we train children you know in terms of accessing the wider world that you might want to start your youngest children out with these very protected environments but as kids get older they need to learn how to deal with risks you know you can't keep an individual in a walled garden a completely safe environment until they're 18 and then say go deal with the world and expect them to manage it well so I think yes for the youngest children we will increasingly see and should see very safe environments which are more maybe human moderated or use algorithms in a very sort of much narrow way but I think for older children I don't think we'll see that and I don't think we should see that. I'd like to thank Dr. Vicky Nash of the Oxford Internet Institute and Andy Burrows of the NSPCC for joining us this week. Before we go it's time for our interesting tech fact of the week. Those who need to monitor their alcohol consumption may one day be able to do so without a breathalyzer or an invasive blood test. Researchers at the University of California have created a prototype of a tiny sensor implanted under the skin that measures the amount of ethanol in the fluid between your cells designed to draw power from a smartwatch. If you have any fun tech facts, questions or feedback on the show and if you have any ideas for cool digital stories that we should cover in future episodes email us at podcasts at the Guardian.com. I'm Jordan Erica Weber see you next time. For more great podcasts from The Guardian just go to the Guardian.com slash podcasts.