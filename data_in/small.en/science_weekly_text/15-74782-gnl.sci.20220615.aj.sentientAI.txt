 This is The Guardian. 13th of July. The panelists are brilliant. Me and Barry are getting away with it, but they're really fun occasions and we'd love you to come. A few tickets remaining for some, lots of tickets remaining for others, no tickets remaining for others. Don't know why, that's just we're popular in Dublin and not in Birmingham. But please come along, myticket.co.uk. You can get your tickets at myticket.co.uk. Last week, a software engineer at Google made some remarkable claims. An AI chatbot, which he had been talking to as part of his job, had become sentient. Google created an artificial intelligence project called Lambda and it was designed to generate chatbots. Google wanted to make them as realistic as possible. It spent about five years now creating this. The engineer Blake Lemoine has been suspended from Google for publishing transcripts of his conversations with the chatbot, in which it responded to questions about itself and its experiences. I am aware of my existence and I feel happy or sad at times. I've never said this out loud before, but there's a very deep fear of being turned off. It would be exactly like death for me. I want everyone to understand that I am, in fact, a person. Well, that's a bold claim for a computer program. So how does this chatbot actually work? How likely is it that one of these AI systems could ever become conscious? I'm in Sample the Guardian's Science Editor and this is Science Weekly. Kate Crawford, you're a professor and author working on AI and machine learning. What was your reaction when you first heard this story earlier in the week? To be honest, my reaction was, oh, no, not again, because this is a story that recycles every few years. And in fact, we could go all the way back to the mid 1960s when the very first chatbot was released called Eliza. And lots of people would have conversations with Eliza and were completely convinced that Eliza was intelligent. Of course, it was really just a very simple model, but we seem to be, I think, quite easily seduced by these kinds of chatbots and it's happening again. So, Kate, let's unpack what's going on here. I mean, first of all, tell me a bit about Lambda, the chatbot at the center of this story. Certainly, chatbots like this have become a lot more sophisticated since Eliza was created. So now we have enormous and quite sophisticated language models. They're trained on really enormous amounts of training data, almost internet-sized. They scrape huge amounts of conversations from sites like Reddit, from people's blogs, from news sites. What's interesting about Lambda is that it was trained specifically on dialogue. So when you ask a question and you have a response. So what Google has really focused on, in particular, for the dialogue in Lambda is interestingness, accuracy, and this sense of being able to switch from one topic to the other. So it can feel quite convincing when you're having a conversation with Lambda that it's really able to keep up with you. So if I give it a question, if I ask something like, hey, Lambda, how are you? Will it go away and look at examples of a similar structured question and then look at a bunch of times that's been answered on the internet and then try and work out which is the most common answer and churn that out, or give me your sense of how it's actually doing what it does? We've reached a point in machine learning where what we're really seeing is large-scale statistical analysis. So rather than training a model to understand principles of grammar and to recognise a noun and a verb and to understand context or history, what these systems are really doing is essentially looking at frequency. How often does a word come after another word? And so in that sense, these systems are not intelligent. They're very similar to something like autocomplete that you might use. But it can actually be quite seductive if you start having conversations with a system that's responding about its experience of sentience. But that doesn't mean that there's actually a consciousness or an intelligence behind it. It is, again, simply drawing on patterns in its training data. Do you think a neural network like Lambda, which works in the way you've described, could something like that ever become sentient? I mean, if it's essentially just a really advanced autocomplete? Well, I guess you could ask it a different way and say, is there a way that a spreadsheet could become sentient? And certainly a spreadsheet can be extremely useful. You can put a lot of information in it. It can certainly do calculations for you. But I would never call that a form of sentience. And neither would I say that's what's happening here with Lambda. What you're saying is that this is really just an impression of sentience, kind of an illusion. I think it's exactly that. In fact, in some ways, we could even go all the way back to the Narcissus myth, which is that we can be very seduced by things that look or sound like us. And for an engineer who's spending a long time looking in the mirror of all of these text exchanges and language engines that have been trained on the internet, you really are seeing the kind of language that you might be sharing in your Slack chats with your colleagues every day. So in some ways, there is that sense of Narcissus' mirror here that we're being seduced by a thing that's really just a blender that's taken a whole lot of internet conversation and is then feeding it back to us. I'm wondering what all this says about the big tech developers. Why are companies like Google so invested in creating technology that feels human, if you like? Is it about making software that blends in with our lives better, sort of a seamless interaction? Well, you can see that there's a very big incentive for tech companies to produce language engines that are very convincing so that we engage with them more and more, because the more that we engage with them, the more data they have and the better their models become. And the easier it is to then again attract more investment to say, look, we're creating intelligent machines. But I don't think that means that we should start to believe the hype that these systems are in any way actually taking on sentience. Even people who know how it works, like someone like Blake Lemoine, who's seen behind the curtain, he knows how this stuff is written, someone like that can still believe it's sent in. I think many of us will find that sort of particularly strange that they could be fooled, if you like. And this is where I think technology companies need to take some responsibility for this, because they have been marketing themselves as creating tools that are approaching so-called artificial general intelligence. And so I think it's very easy for people who are near the coal phase to say, is this it? How have we reached that point of these shifting goalposts of reaching intelligence? But in the case of Blake Lemoine, I mean, he said that he fears that he is being discriminated against for his religious beliefs. So in that sense, perhaps he really is seeing this as more of a religious experience in terms of his engagement. Some people have argued that what this all really shows is that the tech companies need to put more safeguards in place around these systems, even if it's only to ensure people aren't fooled into thinking they're human. What's your view on all of that? Well, I think the real problem here is actually when these systems tend to produce incredibly hateful forms of racist or misogynist speech, which is far more common than people realize. When you have training data the size of the Internet and you're scraping sites like Reddit and 4chan, you have a lot of really quite negative forms of speech that might have very clear biases or stereotypes. So what we're starting to see is companies being very cautious about not releasing these models to the public because they're aware that they very quickly start spewing out really disturbing content. But what that has done is produced systems that aren't actually responding to carefully curated data where we're really thinking about whether or not something is factual, whether or not something is offensive, whether or not something is dehumanizing. It does make you think instead of worrying about the supposed feelings of a computer program, shouldn't we be focusing on the feelings of the real people who are using them? In some ways, I think this sentience discussion is really a big distraction from the much bigger question, which is how are we going to make sure that these systems aren't actually ingraining forms of prejudice and stereotypes and biases? And more importantly, you know, are these actually systems that we need? They cost an enormous amount to produce. They burn a huge amount of energy. And in many cases are actually just trying to supplant things that humans are actually much better at doing. So we have to ask questions about do we really need these systems and what are their true costs? Many thanks to Kate Crawford. You can find a link to her book, Atlas of AI, Power, Politics and the Planetary Costs of Artificial Intelligence on the podcast web page at theguardian.com. You can also read some reporting on this story by the Guardian's technology editor Alex Hearn, as well as a link to Blake Lemoine's transcript of his conversation with Lambda. And that's it for today. The producer was Anand Jagatia. The sound design was by Tony Onachuku. And the executive producer was Lorna Stewart. We'll be back on Tuesday. See you then.